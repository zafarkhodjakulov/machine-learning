{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Dataset and DataLoader in Deep Learning**  \n",
    "\n",
    "In deep learning, **Dataset** and **DataLoader** are fundamental concepts that enable efficient data management, preprocessing, and batching during model training and evaluation. This part of the tutorial will provide a **detailed conceptual understanding** of these topics before we dive into their implementation in later parts.\n",
    "\n",
    "\n",
    "### **Why are Dataset and DataLoader Important in Deep Learning?**\n",
    "\n",
    "Deep learning models learn from data. The way we **store, load, and process** this data greatly affects model performance and training efficiency. A deep learning pipeline often involves **large datasets** that cannot fit entirely into memory. This is where Dataset and DataLoader come in:\n",
    "\n",
    "- **Dataset**: Organizes and provides access to the data.\n",
    "- **DataLoader**: Efficiently loads the data in batches, shuffles, and applies transformations.\n",
    "\n",
    "\n",
    "### **Understanding Datasets in Deep Learning**\n",
    "\n",
    "#### **What is a Dataset?**\n",
    "A **Dataset** is a structured collection of data samples used to train, validate, and test a deep learning model. It typically consists of **features (input data)** and **labels (output data)**.\n",
    "\n",
    "#### **Types of Datasets**\n",
    "1. **Structured Data**\n",
    "   - Tabular data (e.g., CSV files, SQL databases)\n",
    "   - Examples: Titanic dataset, House Price Prediction dataset\n",
    "\n",
    "2. **Unstructured Data**\n",
    "   - **Image Datasets** (e.g., MNIST, CIFAR-10)\n",
    "   - **Text Datasets** (e.g., IMDB movie reviews)\n",
    "   - **Audio Datasets** (e.g., Speech Commands dataset)\n",
    "   - **Video Datasets** (e.g., Activity recognition datasets)\n",
    "\n",
    "#### **Dataset Storage Formats**\n",
    "Datasets can be stored in different formats:\n",
    "- **CSV, JSON, XML** (common for structured data)\n",
    "- **Images (PNG, JPG), Audio (WAV, MP3), Video (MP4)**\n",
    "- **HDF5, TFRecord, NumPy files (.npy, .npz)** for efficient storage and retrieval\n",
    "\n",
    "#### **Challenges in Handling Datasets**\n",
    "- **Size:** Large datasets may not fit in memory.\n",
    "- **Variability:** Data may have missing values, inconsistent formats, or noise.\n",
    "- **Preprocessing:** Requires transformations like normalization, augmentation, and encoding.\n",
    "- **Imbalance:** Some classes may have more samples than others, affecting model performance.\n",
    "\n",
    "### **Understanding DataLoader in Deep Learning**\n",
    "\n",
    "#### **What is a DataLoader?**\n",
    "A **DataLoader** is a utility that **loads data efficiently** during training. Instead of loading all data at once, it loads small batches, applies transformations, and enables parallel data processing.\n",
    "\n",
    "#### **Why Use a DataLoader?**\n",
    "1. **Memory Efficiency**: Loads only small batches into memory, avoiding memory overflow.\n",
    "2. **Performance Boost**: Uses multiple workers (multi-threading) to speed up data loading.\n",
    "3. **Shuffling**: Ensures the model does not memorize the data sequence.\n",
    "4. **Batching**: Loads multiple samples at once to improve GPU utilization.\n",
    "5. **Transformations**: Applies preprocessing like normalization and augmentation on-the-fly.\n",
    "\n",
    "\n",
    "### **Key Concepts in DataLoader**\n",
    "\n",
    "#### **A. Batch Size**\n",
    "- The number of samples loaded at a time.\n",
    "- Example:\n",
    "  - **Batch size = 32** → 32 images are loaded per iteration.\n",
    "  - Larger batch sizes require more memory but improve model convergence.\n",
    "\n",
    "#### **B. Shuffling**\n",
    "- Randomly rearranges the dataset each epoch to prevent the model from learning the data order.\n",
    "- Important for training but usually disabled for evaluation.\n",
    "\n",
    "#### **C. Num Workers**\n",
    "- Specifies how many parallel processes should load the data.\n",
    "- **num_workers = 0** → Uses the main process (slower).\n",
    "- **num_workers > 0** → Uses multiple threads (faster).\n",
    "\n",
    "#### **D. Pin Memory**\n",
    "- Helps speed up data transfer to the GPU by storing tensors in **pinned memory (RAM).**\n",
    "- Enabled using `pin_memory=True`.\n",
    "\n",
    "### **How Dataset and DataLoader Work Together**\n",
    "1. The **Dataset** object loads and processes individual data samples.\n",
    "2. The **DataLoader** object retrieves data from the Dataset in batches, applies preprocessing, and passes it to the model.\n",
    "\n",
    "#### **Example Workflow**\n",
    "1. Define a **Dataset** (e.g., load images from a folder).\n",
    "2. Pass the Dataset to a **DataLoader**.\n",
    "3. The DataLoader loads batches, applies transformations, and feeds the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset and DataLoader from Scratch**\n",
    "\n",
    "Now that we understand the concepts behind **Dataset** and **DataLoader**, let's implement them **manually** using pure Python. We'll cover three types of data:\n",
    "1. **Tabular Data** (CSV files)\n",
    "2. **Text Data** (NLP datasets)\n",
    "3. **Image Data** (loading images from directories)\n",
    "\n",
    "\n",
    "### **Implementing a Dataset Class from Scratch**\n",
    "A **Dataset** should:\n",
    "- Load data from a storage format (CSV, text files, images).\n",
    "- Apply preprocessing (optional).\n",
    "- Provide access to individual samples using `__getitem__`.\n",
    "\n",
    "#### **Base Dataset Class**\n",
    "We'll create a base class for all datasets.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Base dataset class that loads data from a given path.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Override this method to load data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Override this method to access individual data points.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "```\n",
    "\n",
    "### **2. Implementing a DataLoader from Scratch**\n",
    "A **DataLoader** should:\n",
    "- Load data in batches.\n",
    "- Shuffle data if needed.\n",
    "- Provide an iterator to access batches.\n",
    "\n",
    "```python\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        \"\"\"\n",
    "        Custom DataLoader class to handle batching and shuffling.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = list(range(len(dataset)))\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over batches.\n",
    "        \"\"\"\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Fetches the next batch of data.\n",
    "        \"\"\"\n",
    "        if self.current_index >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        start = self.current_index\n",
    "        end = min(self.current_index + self.batch_size, len(self.dataset))\n",
    "        batch = [self.dataset[i] for i in self.indices[start:end]]\n",
    "        self.current_index = end\n",
    "        return batch\n",
    "```\n",
    "\n",
    "## **3. Implementing Different Dataset Types**\n",
    "\n",
    "#### **A. Tabular Dataset (CSV)**\n",
    "We will implement a dataset class that loads **tabular data** from a CSV file.\n",
    "\n",
    "##### **Example CSV File (data.csv)**\n",
    "```\n",
    "age, salary, label\n",
    "25, 50000, 1\n",
    "30, 60000, 0\n",
    "35, 70000, 1\n",
    "40, 80000, 0\n",
    "```\n",
    "\n",
    "##### **Implementation**\n",
    "```python\n",
    "import csv\n",
    "\n",
    "class TabularDataset(CustomDataset):\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load tabular data from a CSV file.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with open(self.data_path, \"r\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                age, salary, label = map(float, row)\n",
    "                data.append((age, salary, label))\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a single data sample.\n",
    "        \"\"\"\n",
    "        return self.data[index]\n",
    "```\n",
    "\n",
    "##### **Testing with DataLoader**\n",
    "```python\n",
    "dataset = TabularDataset(\"data.csv\")\n",
    "dataloader = CustomDataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Text Dataset (NLP)**\n",
    "We will implement a dataset class that loads **text data** from a file.\n",
    "\n",
    "##### **Example Text File (text_data.txt)**\n",
    "```\n",
    "Hello world!\n",
    "Machine learning is powerful.\n",
    "Deep learning improves AI.\n",
    "```\n",
    "\n",
    "##### **Implementation**\n",
    "```python\n",
    "class TextDataset(CustomDataset):\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load text data from a file.\n",
    "        \"\"\"\n",
    "        with open(self.data_path, \"r\") as file:\n",
    "            return [line.strip() for line in file.readlines()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a single line of text.\n",
    "        \"\"\"\n",
    "        return self.data[index]\n",
    "```\n",
    "\n",
    "##### **Testing with DataLoader**\n",
    "```python\n",
    "dataset = TextDataset(\"text_data.txt\")\n",
    "dataloader = CustomDataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "```\n",
    "\n",
    "#### **C. Image Dataset**\n",
    "We will implement a dataset class that loads **images** from a directory.\n",
    "\n",
    "#### **Implementation**\n",
    "```python\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(CustomDataset):\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load image file paths from a directory.\n",
    "        \"\"\"\n",
    "        return [os.path.join(self.data_path, f) for f in os.listdir(self.data_path) if f.endswith((\".png\", \".jpg\"))]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load and return an image.\n",
    "        \"\"\"\n",
    "        img_path = self.data[index]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        return image\n",
    "```\n",
    "\n",
    "##### **Testing with DataLoader**\n",
    "```python\n",
    "dataset = ImageDataset(\"images/\")\n",
    "dataloader = CustomDataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset and DataLoader using PyTorch**\n",
    "\n",
    "Now that we have implemented Dataset and DataLoader from scratch, let's see how PyTorch simplifies these tasks using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "\n",
    "### **1. Understanding PyTorch Dataset and DataLoader**\n",
    "\n",
    "#### **PyTorch Dataset (`torch.utils.data.Dataset`)**\n",
    "- A custom dataset in PyTorch is created by subclassing `torch.utils.data.Dataset`.\n",
    "- It must implement:\n",
    "  - `__init__`: Initialize dataset (e.g., load files, store paths, read CSVs).\n",
    "  - `__len__`: Return the total number of samples.\n",
    "  - `__getitem__`: Retrieve a sample by index.\n",
    "\n",
    "#### **PyTorch DataLoader (`torch.utils.data.DataLoader`)**\n",
    "- Wraps around a `Dataset` to enable efficient data loading.\n",
    "- Key features:\n",
    "  - **Batching**: Loads data in batches.\n",
    "  - **Shuffling**: Randomizes data order.\n",
    "  - **Multiprocessing**: Uses multiple workers for speed.\n",
    "  - **Transforms**: Applies preprocessing on-the-fly.\n",
    "\n",
    "\n",
    "### **2. Implementing Dataset for Different Data Types**\n",
    "\n",
    "#### **A. Tabular Dataset (CSV)**\n",
    "\n",
    "##### **Example CSV File (data.csv)**\n",
    "```\n",
    "age,salary,label\n",
    "25,50000,1\n",
    "30,60000,0\n",
    "35,70000,1\n",
    "40,80000,0\n",
    "```\n",
    "\n",
    "##### **Implementation**\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        features = torch.tensor([row['age'], row['salary']], dtype=torch.float32)\n",
    "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
    "        return features, label\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TabularDataset(\"data.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "```\n",
    "\n",
    "#### **B. Text Dataset (NLP)**\n",
    "\n",
    "##### **Example Text File (text_data.txt)**\n",
    "```\n",
    "Hello world!\n",
    "Machine learning is powerful.\n",
    "Deep learning improves AI.\n",
    "```\n",
    "\n",
    "##### **Implementation**\n",
    "```python\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_file):\n",
    "        with open(text_file, \"r\") as file:\n",
    "            self.data = file.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index].strip()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(\"text_data.txt\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "```\n",
    "\n",
    "#### **C. Image Dataset**\n",
    "\n",
    "##### **Implementation**\n",
    "```python\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImageDataset(\"images/\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
