{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running colab? False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "is_colab = True\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "except ModuleNotFoundError:\n",
    "    is_colab = False\n",
    "\n",
    "print(\"Is running colab?\", is_colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ai-roadmap\\content\\dl\\language_modelling\\lstm\\data\n"
     ]
    }
   ],
   "source": [
    "if is_colab:\n",
    "    data_dir = Path('/content/drive/MyDrive/data')\n",
    "    code_dir = Path('/content/drive/MyDrive/code')\n",
    "    models_dir = Path('/content/drive/MyDrive/models/rnn')\n",
    "else:\n",
    "    data_dir = Path('data').resolve()\n",
    "    models_dir = Path('models').resolve()\n",
    "\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    import shutil\n",
    "    shutil.copy(code_dir / 'rnn/dataset.py',\n",
    "                'dataset.py')\n",
    "\n",
    "    shutil.copy(code_dir / 'rnn/chars.json',\n",
    "                'chars.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import get_all_stories, get_dataset, CharacterLevelTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, val_stories, test_stories = get_all_stories(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterLevelTokenizer()\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=8\n",
    "Ds_tr = get_dataset(train_stories, tokenizer, block_size=block_size)\n",
    "Ds_val = get_dataset(val_stories, tokenizer, block_size=block_size)\n",
    "Ds_test = get_dataset(test_stories, tokenizer, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = Ds_tr.tensors\n",
    "Xval, Yval = Ds_val.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 14,  2,  ..., 35, 18, 27],\n",
      "        [14,  2, 31,  ..., 18, 27,  2],\n",
      "        [ 2, 31, 14,  ..., 27,  2, 14],\n",
      "        ...,\n",
      "        [46,  2, 55,  ..., 45, 58, 59],\n",
      "        [ 2, 55, 60,  ..., 58, 59,  8],\n",
      "        [55, 60, 48,  ..., 59,  8, 40]])\n",
      "tensor([[14,  2, 31,  ..., 18, 27,  2],\n",
      "        [ 2, 31, 14,  ..., 27,  2, 14],\n",
      "        [31, 14, 35,  ...,  2, 14, 27],\n",
      "        ...,\n",
      "        [ 2, 55, 60,  ..., 58, 59,  8],\n",
      "        [55, 60, 48,  ..., 59,  8, 40],\n",
      "        [60, 48, 45,  ...,  8, 40,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr)\n",
    "print(Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tr_loader = DataLoader(Ds_tr, batch_size=4)\n",
    "Val_loader = DataLoader(Ds_val, batch_size=4)\n",
    "Test_loader = DataLoader(Ds_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_loss(model, X, Y):\n",
    "    model.eval()\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    # logits = model(X)\n",
    "    # loss = F.cross_entropy(logits, F.one_hot(Y, vocab_size).float())\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    loss = F.cross_entropy(logits, Y.view(-1))\n",
    "\n",
    "    # for param in bigram_model.parameters():\n",
    "    #     loss += param.pow(2).mean().sqrt()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_dim = n_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(vocab_size, n_dim)\n",
    "        self.rnn = nn.LSTM(n_dim, n_hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     h0 = self.init_hidden(x)\n",
    "    #     out = self.emb(x)\n",
    "    #     out, _ = self.rnn(out, h0)\n",
    "    #     out = torch.tanh(out)\n",
    "    #     out = self.fc(out)\n",
    "    #     return out\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        h0 = h0 if h0 is not None else self.init_hidden(x)\n",
    "        out = self.emb(x)\n",
    "        out, hn = self.rnn(out, h0)\n",
    "        out = torch.tanh(out)\n",
    "        out = self.fc(out)\n",
    "        return out, hn\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        b, t = x.shape\n",
    "        return torch.zeros(1, b, self.n_hidden).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size, n_dim=32, n_hidden=100).to(device)\n",
    "optimizer = optim.SGD(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (emb): Embedding(68, 32)\n",
      "  (rnn): LSTM(32, 100, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=68, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, optimizer, criterion, save_path):\n",
    "    all_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        lossi = []\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Initialize hidden state\n",
    "            h0 = model.init_hidden(x)  # (num_layers=1, batch_size, hidden_size)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(x, h0)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lossi.append(loss.item())\n",
    "\n",
    "        all_loss.extend(lossi)\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), save_path / f'model_epoch{epoch+1}.pth')\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, \"\n",
    "            f\"Avg Loss: {torch.tensor(lossi).mean():.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, epochs, optimizer, criterion, save_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m h0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(x)  \u001b[38;5;66;03m# (num_layers=1, batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x, h0)\u001b[0m\n\u001b[0;32m     20\u001b[0m h0 \u001b[38;5;241m=\u001b[39m h0 \u001b[38;5;28;01mif\u001b[39;00m h0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(x)\n\u001b[0;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[1;32m---> 22\u001b[0m out, hn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(out)\n\u001b[0;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1106\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m   1104\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1107\u001b[0m         )\n\u001b[0;32m   1108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "train(model, Tr_loader, epochs=2, optimizer=optimizer, criterion=criterion, save_path=models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "plt.plot(torch.tensor(all_loss[:len(all_loss) // n * n]).view(-1, n).mean(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, block_size=8, start_token=0, generator=None, k=None, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize context window with `block_size` start tokens\n",
    "    context = [start_token] * block_size  \n",
    "    out = []\n",
    "\n",
    "    for _ in range(max_length):  # Prevent infinite loops\n",
    "        # Convert to tensor and ensure correct shape (batch_size=1)\n",
    "        input_tensor = torch.tensor([context], dtype=torch.long).to(next(model.parameters()).device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(input_tensor)[:, -1, :]  # Get logits for last token\n",
    "\n",
    "        # Apply top-k filtering if needed\n",
    "        if k:\n",
    "            top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "            top_k_mask = torch.full_like(logits, float('-inf'))\n",
    "            top_k_mask.scatter_(1, top_k_indices, top_k_values)\n",
    "            logits = top_k_mask\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        p = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample next token\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()\n",
    "\n",
    "        if ix == start_token:\n",
    "            break  # Stop if we reach the start token (assuming it's also the EOS token)\n",
    "\n",
    "        out.append(ix)\n",
    "\n",
    "        # Update context window (keep last `block_size` tokens)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(42)\n",
    "ids = generate(model, generator=g, k=10, max_length=2000)\n",
    "\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = generate(model, k=15, max_length=2000)\n",
    "\n",
    "print(tokenizer.decode(ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
