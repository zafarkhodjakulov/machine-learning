{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;color:#0F4C81;\">\n",
    "PyTorch Code Module\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is not the only library that deals with multidimensional arrays. NumPy is by far the most popular multidimensional array library. PyTorch features seamless interoperability with NumPy, which brings with it first-class integration with the rest of the scientific libraries in Python, such as SciPy, Scikit-learn and Pandas.\n",
    "\n",
    "Compared to NumPy arrays, PyTorch tensors have a few superpowers, such as the ability to perform very fast operations on graphical processing units (GPUs), distribute operations on multiple devices or machines, and keep track of the graph of computations that created them. These are all important features when implementing a modern deep learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Creating Tensors**\n",
    "\n",
    "PyTorch provides various ways to create tensors, from manually specifying data to using built-in functions for initialization. Below are some common methods:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Creating a Tensor from Data**\n",
    "You can create a tensor manually by passing a Python list or NumPy array to `torch.tensor()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor from a list\n",
    "tensor1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(tensor1)\n",
    "\n",
    "# Creating a tensor with floating-point values\n",
    "tensor2 = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(tensor2)\n",
    "\n",
    "# Specifying a data type explicitly\n",
    "tensor3 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Initializing Tensors**\n",
    "You can create tensors with default values using different functions:\n",
    "\n",
    "- `torch.zeros(size)`: Creates a tensor filled with zeros.\n",
    "- `torch.ones(size)`: Creates a tensor filled with ones.\n",
    "- `torch.rand(size)`: Creates a tensor with random values from a uniform distribution (`[0,1)`).\n",
    "- `torch.randn(size)`: Creates a tensor with random values from a normal distribution (mean=0, std=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros Tensor:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Ones Tensor:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "Random Tensor (Uniform):\n",
      " tensor([[0.3867, 0.5878],\n",
      "        [0.3218, 0.0407]])\n",
      "Random Tensor (Normal):\n",
      " tensor([[-0.9435,  0.0516, -0.6080],\n",
      "        [-0.0529, -0.1738,  0.5972],\n",
      "        [-1.8542, -1.2465, -1.6267]])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors with predefined values\n",
    "zeros_tensor = torch.zeros(3, 3)  # 3x3 tensor filled with zeros\n",
    "ones_tensor = torch.ones(2, 4)    # 2x4 tensor filled with ones\n",
    "rand_tensor = torch.rand(2, 2)    # 2x2 tensor with random values in [0,1)\n",
    "randn_tensor = torch.randn(3, 3)  # 3x3 tensor with normal distribution values\n",
    "\n",
    "print(\"Zeros Tensor:\\n\", zeros_tensor)\n",
    "print(\"Ones Tensor:\\n\", ones_tensor)\n",
    "print(\"Random Tensor (Uniform):\\n\", rand_tensor)\n",
    "print(\"Random Tensor (Normal):\\n\", randn_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Creating Sequences**\n",
    "You can create sequences using `torch.arange()` and `torch.linspace()`.\n",
    "\n",
    "- `torch.arange(start, end, step)`: Creates a tensor with values in a specified range.\n",
    "- `torch.linspace(start, end, steps)`: Creates a tensor with `steps` number of evenly spaced values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range Tensor: tensor([0, 2, 4, 6, 8])\n",
      "Linspace Tensor: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Creating a range of values with arange\n",
    "range_tensor = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "print(\"Range Tensor:\", range_tensor)\n",
    "\n",
    "# Creating evenly spaced values with linspace\n",
    "linspace_tensor = torch.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "print(\"Linspace Tensor:\", linspace_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 Special Tensors**\n",
    "PyTorch provides functions for creating special tensors:\n",
    "\n",
    "- `torch.eye(n)`: Creates an identity matrix of size `n Ã— n`.\n",
    "- `torch.full(size, value)`: Creates a tensor of a given shape, filled with a specified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Tensor:\n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Full Tensor:\n",
      " tensor([[7, 7, 7],\n",
      "        [7, 7, 7],\n",
      "        [7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Creating an identity matrix\n",
    "identity_tensor = torch.eye(4)  # 4x4 identity matrix\n",
    "print(\"Identity Tensor:\\n\", identity_tensor)\n",
    "\n",
    "# Creating a tensor filled with a constant value\n",
    "full_tensor = torch.full((3, 3), 7)  # 3x3 tensor filled with 7s\n",
    "print(\"Full Tensor:\\n\", full_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Reshaping and Changing Dimensions**\n",
    "\n",
    "Reshaping tensors is a crucial part of working with PyTorch. This section covers various operations to modify tensor dimensions while keeping data intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Squeeze & Unsqueeze**\n",
    "- `torch.squeeze(tensor)`: Removes dimensions of size 1.\n",
    "- `torch.unsqueeze(tensor, dim)`: Adds a new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Shape: torch.Size([1, 3, 1, 4])\n",
      "Squeezed Tensor Shape: torch.Size([3, 4])\n",
      "Squeezed Tensor (dim=2) Shape: torch.Size([1, 3, 4])\n",
      "Unsqueezed Tensor Shape: torch.Size([1, 1, 3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor with extra dimensions\n",
    "tensor = torch.rand(1, 3, 1, 4)\n",
    "print(\"Original Tensor Shape:\", tensor.shape)\n",
    "\n",
    "# Removing dimensions of size 1\n",
    "squeezed_tensor = torch.squeeze(tensor)  # Removes all size-1 dimensions\n",
    "print(\"Squeezed Tensor Shape:\", squeezed_tensor.shape)\n",
    "\n",
    "# Removing only a specific dimension\n",
    "squeezed_tensor_dim2 = torch.squeeze(tensor, dim=2)\n",
    "print(\"Squeezed Tensor (dim=2) Shape:\", squeezed_tensor_dim2.shape)\n",
    "\n",
    "# Adding a new dimension\n",
    "unsqueezed_tensor = torch.unsqueeze(tensor, dim=0)\n",
    "print(\"Unsqueezed Tensor Shape:\", unsqueezed_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 View & Reshape**\n",
    "- `tensor.view(shape)`: Changes the shape without modifying data layout (requires contiguous memory).\n",
    "- `tensor.reshape(shape)`: Similar to `view`, but can create a copy if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Reshaped Tensor (2x4):\n",
      " tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "Reshaped Tensor (4x2):\n",
      " tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 2D tensor\n",
    "tensor = torch.arange(8)\n",
    "print(\"Original Tensor:\", tensor)\n",
    "\n",
    "# Reshaping it to 2x4\n",
    "reshaped_tensor = tensor.view(2, 4)\n",
    "print(\"Reshaped Tensor (2x4):\\n\", reshaped_tensor)\n",
    "\n",
    "# Reshaping using reshape() (safe for non-contiguous memory)\n",
    "reshaped_tensor2 = tensor.reshape(4, 2)\n",
    "print(\"Reshaped Tensor (4x2):\\n\", reshaped_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Transpose & Permute**\n",
    "- `tensor.T`: Swaps the last two dimensions (shortcut for 2D matrices).\n",
    "- `tensor.transpose(dim0, dim1)`: Swaps any two dimensions.\n",
    "- `tensor.permute(*dims)`: Rearranges dimensions arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix Shape: torch.Size([3, 4])\n",
      "Transposed Matrix Shape: torch.Size([4, 3])\n",
      "Transposed Tensor Shape: torch.Size([2, 4, 3])\n",
      "Permuted Tensor Shape: torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 2D matrix\n",
    "matrix = torch.rand(3, 4)\n",
    "print(\"Original Matrix Shape:\", matrix.shape)\n",
    "\n",
    "# Transposing the matrix (shortcut for 2D)\n",
    "transposed_matrix = matrix.T\n",
    "print(\"Transposed Matrix Shape:\", transposed_matrix.shape)\n",
    "\n",
    "# Swapping two specific dimensions\n",
    "tensor_3d = torch.rand(2, 3, 4)\n",
    "transposed_tensor = tensor_3d.transpose(1, 2)\n",
    "print(\"Transposed Tensor Shape:\", transposed_tensor.shape)\n",
    "\n",
    "# Permuting dimensions\n",
    "permuted_tensor = tensor_3d.permute(2, 0, 1)\n",
    "print(\"Permuted Tensor Shape:\", permuted_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Flatten & Expand**\n",
    "- `tensor.flatten(start_dim, end_dim)`: Converts tensor to 1D (or partially flattens).\n",
    "- `tensor.expand(new_shape)`: Expands dimensions without copying data.\n",
    "- `tensor.repeat(repeats)`: Repeats data (creates a new memory allocation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Tensor Shape: torch.Size([120])\n",
      "Partially Flattened Tensor Shape: torch.Size([2, 12, 5])\n",
      "Expanded Tensor Shape: torch.Size([5, 3])\n",
      "Repeated Tensor Shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a multi-dimensional tensor\n",
    "tensor_4d = torch.rand(2, 3, 4, 5)\n",
    "\n",
    "# Flattening the tensor completely\n",
    "flattened_tensor = tensor_4d.flatten()\n",
    "print(\"Flattened Tensor Shape:\", flattened_tensor.shape)\n",
    "\n",
    "# Flattening only a specific range of dimensions\n",
    "partially_flattened_tensor = tensor_4d.flatten(start_dim=1, end_dim=2)\n",
    "print(\"Partially Flattened Tensor Shape:\", partially_flattened_tensor.shape)\n",
    "\n",
    "# Expanding a tensor (without copying data)\n",
    "tensor_exp = torch.rand(1, 3)\n",
    "expanded_tensor = tensor_exp.expand(5, 3)  # Expands along the first dimension\n",
    "print(\"Expanded Tensor Shape:\", expanded_tensor.shape)\n",
    "\n",
    "# Repeating a tensor (allocates new memory)\n",
    "repeated_tensor = tensor_exp.repeat(2, 1)\n",
    "print(\"Repeated Tensor Shape:\", repeated_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Indexing & Slicing**\n",
    "- `tensor[idx]`: Basic indexing\n",
    "- `tensor[start:end]`: Slicing\n",
    "- `tensor[..., idx]`: Select last dimension\n",
    "- `tensor.masked_select(mask)`: Select elements based on condition\n",
    "- `tensor.index_select(dim, indices)`: Select elements along a specific dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Indexing & Slicing**\n",
    "\n",
    "Indexing and slicing allow us to extract specific elements or sub-tensors from a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Basic Indexing**\n",
    "You can access elements in a tensor using standard Python indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[10, 20, 30],\n",
      "        [40, 50, 60],\n",
      "        [70, 80, 90]])\n",
      "Element at (0,1): tensor(20)\n",
      "First Row: tensor([10, 20, 30])\n",
      "First Column: tensor([10, 40, 70])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor\n",
    "tensor = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n",
    "print(\"Original Tensor:\\n\", tensor)\n",
    "\n",
    "# Accessing a single element\n",
    "print(\"Element at (0,1):\", tensor[0, 1])  # Output: 20\n",
    "\n",
    "# Accessing an entire row\n",
    "print(\"First Row:\", tensor[0])\n",
    "\n",
    "# Accessing an entire column\n",
    "print(\"First Column:\", tensor[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Slicing**\n",
    "Slicing allows selecting specific ranges from tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-Tensor:\n",
      " tensor([[20, 30],\n",
      "        [50, 60]])\n",
      "Tensor with Step:\n",
      " tensor([[10, 30],\n",
      "        [40, 60],\n",
      "        [70, 90]])\n"
     ]
    }
   ],
   "source": [
    "# Selecting a sub-tensor\n",
    "sub_tensor = tensor[0:2, 1:]  # Rows 0 to 1, columns 1 to end\n",
    "print(\"Sub-Tensor:\\n\", sub_tensor)\n",
    "\n",
    "# Using step in slicing\n",
    "stepped_tensor = tensor[:, ::2]  # Select every second column\n",
    "print(\"Tensor with Step:\\n\", stepped_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Ellipsis (`...`) for Slicing**\n",
    "Ellipsis (`...`) is useful for selecting elements in high-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Shape: torch.Size([2, 3, 4])\n",
      "Selected Last Dimension:\n",
      " tensor([[0.7568, 0.9949, 0.0275],\n",
      "        [0.9074, 0.6104, 0.9144]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 3D tensor\n",
    "tensor_3d = torch.rand(2, 3, 4)\n",
    "print(\"3D Tensor Shape:\", tensor_3d.shape)\n",
    "\n",
    "# Using ellipsis to select the last dimension\n",
    "print(\"Selected Last Dimension:\\n\", tensor_3d[..., -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Masked Indexing**\n",
    "Masked indexing allows selecting elements based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements > 20: tensor([25, 35, 45])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "tensor = torch.tensor([5, 15, 25, 35, 45])\n",
    "\n",
    "# Selecting elements greater than 20\n",
    "mask = tensor > 20\n",
    "selected_elements = tensor[mask]\n",
    "print(\"Elements > 20:\", selected_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5 Index Select**\n",
    "`torch.index_select()` selects elements along a specific dimension using an index tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Rows:\n",
      " tensor([[10, 20, 30],\n",
      "        [70, 80, 90]])\n",
      "Selected Columns:\n",
      " tensor([[20, 30],\n",
      "        [50, 60],\n",
      "        [80, 90]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "tensor = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n",
    "\n",
    "# Selecting specific rows\n",
    "indices = torch.tensor([0, 2])  # Selecting first and last row\n",
    "selected_rows = torch.index_select(tensor, dim=0, index=indices)\n",
    "print(\"Selected Rows:\\n\", selected_rows)\n",
    "\n",
    "# Selecting specific columns\n",
    "indices = torch.tensor([1, 2])  # Selecting second and third columns\n",
    "selected_columns = torch.index_select(tensor, dim=1, index=indices)\n",
    "print(\"Selected Columns:\\n\", selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combining Tensors**\n",
    "\n",
    "PyTorch provides several ways to combine tensors, including concatenation, stacking, and tiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Concatenation**\n",
    "- `torch.cat(tensors, dim)`: Concatenates tensors along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along Rows:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "Concatenated along Columns:\n",
      " tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating two tensors\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Concatenating along rows (dim=0)\n",
    "concat_rows = torch.cat((tensor1, tensor2), dim=0)\n",
    "print(\"Concatenated along Rows:\\n\", concat_rows)\n",
    "\n",
    "# Concatenating along columns (dim=1)\n",
    "concat_cols = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(\"Concatenated along Columns:\\n\", concat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Stacking**\n",
    "- `torch.stack(tensors, dim)`: Creates a new dimension and stacks tensors along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Tensor Shape: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Stacking along a new dimension\n",
    "stacked_tensor = torch.stack((tensor1, tensor2), dim=0)\n",
    "print(\"Stacked Tensor Shape:\", stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Repeat & Expand**\n",
    "- `torch.repeat(repeats)`: Repeats a tensor along specified dimensions (allocates new memory).\n",
    "- `torch.expand(new_shape)`: Expands a tensor without copying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Tensor:\n",
      " tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [1, 2, 1, 2, 1, 2]])\n",
      "Expanded Tensor:\n",
      " tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 1D tensor\n",
    "tensor = torch.tensor([[1, 2]])\n",
    "\n",
    "# Repeating along rows and columns\n",
    "repeated_tensor = tensor.repeat(2, 3)\n",
    "print(\"Repeated Tensor:\\n\", repeated_tensor)\n",
    "\n",
    "# Expanding a tensor\n",
    "expanded_tensor = tensor.expand(3, 2)  # Expands without copying\n",
    "print(\"Expanded Tensor:\\n\", expanded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4 Meshgrid for Combination**\n",
    "- `torch.meshgrid(tensors)`: Generates coordinate grids from 1D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid X:\n",
      " tensor([[1, 1],\n",
      "        [2, 2],\n",
      "        [3, 3]])\n",
      "Grid Y:\n",
      " tensor([[4, 5],\n",
      "        [4, 5],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Creating coordinate ranges\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5])\n",
    "\n",
    "# Generating a meshgrid\n",
    "grid_x, grid_y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "print(\"Grid X:\\n\", grid_x)\n",
    "print(\"Grid Y:\\n\", grid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Mathematical Operations on Tensors**\n",
    "\n",
    "PyTorch supports various element-wise, matrix, and reduction operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Element-wise Operations**\n",
    "These operations are performed on corresponding elements of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: tensor([5., 7., 9.])\n",
      "Subtraction: tensor([-3., -3., -3.])\n",
      "Multiplication: tensor([ 4., 10., 18.])\n",
      "Division: tensor([0.2500, 0.4000, 0.5000])\n",
      "Exponentiation: tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating two tensors\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Basic arithmetic operations\n",
    "print(\"Addition:\", a + b)\n",
    "print(\"Subtraction:\", a - b)\n",
    "print(\"Multiplication:\", a * b)\n",
    "print(\"Division:\", a / b)\n",
    "print(\"Exponentiation:\", a ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also provides equivalent functions:\n",
    "- `torch.add(a, b)`, `torch.sub(a, b)`, `torch.mul(a, b)`, `torch.div(a, b)`, `torch.pow(a, exponent)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.add: tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Using functions\n",
    "print(\"Using torch.add:\", torch.add(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Matrix Operations**\n",
    "- `torch.mm(A, B)`: Matrix multiplication.\n",
    "- `torch.matmul(A, B)`: Generalized matrix multiplication.\n",
    "- `A @ B`: Shorthand for `torch.matmul(A, B)`.\n",
    "- `torch.dot(a, b)`: Dot product for 1D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication (A @ B):\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Dot Product: tensor(32)\n"
     ]
    }
   ],
   "source": [
    "# Creating matrices\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"Matrix Multiplication (A @ B):\\n\", A @ B)\n",
    "\n",
    "# Dot product\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "print(\"Dot Product:\", torch.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 Reduction Operations**\n",
    "Reduction operations aggregate values along a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: tensor(21)\n",
      "Mean: tensor(3.5000)\n",
      "Standard Deviation: tensor(1.8708)\n",
      "Min: tensor(1)\n",
      "Max: tensor(6)\n",
      "Argmax (Index of max): tensor(5)\n",
      "Argmin (Index of min): tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Summation\n",
    "print(\"Sum:\", torch.sum(tensor))\n",
    "\n",
    "# Mean and standard deviation\n",
    "print(\"Mean:\", torch.mean(tensor.float()))\n",
    "print(\"Standard Deviation:\", torch.std(tensor.float()))\n",
    "\n",
    "# Min and max\n",
    "print(\"Min:\", torch.min(tensor))\n",
    "print(\"Max:\", torch.max(tensor))\n",
    "print(\"Argmax (Index of max):\", torch.argmax(tensor))\n",
    "print(\"Argmin (Index of min):\", torch.argmin(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4 Clamping and Rounding**\n",
    "- `torch.clamp(tensor, min, max)`: Restricts values within a range.\n",
    "- `torch.round(tensor)`, `torch.floor(tensor)`, `torch.ceil(tensor)`: Rounding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([  6.4515,   4.0040,  -8.2213,  -3.3191, -12.6544])\n",
      "Clamped Tensor: tensor([ 5.0000,  4.0040, -5.0000, -3.3191, -5.0000])\n",
      "Rounded: tensor([  6.,   4.,  -8.,  -3., -13.])\n",
      "Floored: tensor([  6.,   4.,  -9.,  -4., -13.])\n",
      "Ceiled: tensor([  7.,   5.,  -8.,  -3., -12.])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor with random values\n",
    "tensor = torch.randn(5) * 10\n",
    "print(\"Original Tensor:\", tensor)\n",
    "\n",
    "# Clamping values between -5 and 5\n",
    "clamped_tensor = torch.clamp(tensor, -5, 5)\n",
    "print(\"Clamped Tensor:\", clamped_tensor)\n",
    "\n",
    "# Rounding operations\n",
    "print(\"Rounded:\", torch.round(tensor))\n",
    "print(\"Floored:\", torch.floor(tensor))\n",
    "print(\"Ceiled:\", torch.ceil(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Logical Operations**\n",
    "- `torch.eq(a, b)`, `torch.ne(a, b)`, `torch.gt(a, b)`, `torch.lt(a, b)`\n",
    "- `tensor.any()`, `tensor.all()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Broadcasting and Tensor Operations with Different Shapes**\n",
    "\n",
    "Broadcasting allows PyTorch to perform operations on tensors of different shapes without explicit expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Broadcasting Basics**\n",
    "Broadcasting follows these rules:\n",
    "1. If tensors have different dimensions, the smaller one is expanded by adding dimensions on the left.\n",
    "2. If sizes differ, the dimension with size `1` is expanded to match the larger one.\n",
    "3. If dimensions are incompatible, an error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar + Vector: tensor([4, 5, 6])\n",
      "Matrix + Vector:\n",
      " tensor([[2, 4, 6],\n",
      "        [5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar and vector operation (scalar is broadcasted)\n",
    "scalar = torch.tensor(3)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"Scalar + Vector:\", scalar + vector)\n",
    "\n",
    "# Matrix and vector broadcasting\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Matrix + Vector:\\n\", matrix + vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Explicit Expansion**\n",
    "- `torch.unsqueeze(tensor, dim)`: Adds a new dimension.\n",
    "- `torch.expand(shape)`: Expands a tensor without copying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Vector:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "Expanded Matrix:\n",
      " tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 1D tensor\n",
    "a = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Unsqueeze to make it a column vector\n",
    "a_column = a.unsqueeze(1)  # Shape: (3,1)\n",
    "print(\"Column Vector:\\n\", a_column)\n",
    "\n",
    "# Expanding to a 3x3 matrix\n",
    "expanded = a_column.expand(3, 3)\n",
    "print(\"Expanded Matrix:\\n\", expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Adding New Dimensions with `None`**\n",
    "Using `None` in indexing adds a new axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with New Axis:\n",
      " tensor([[1, 2, 3]]) Shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a 1D tensor\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Adding an extra dimension\n",
    "tensor_2d = tensor[None, :]\n",
    "print(\"Tensor with New Axis:\\n\", tensor_2d, \"Shape:\", tensor_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4 Broadcasting in Arithmetic Operations**\n",
    "When performing operations on mismatched shapes, broadcasting is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted Addition:\n",
      " tensor([[2, 4, 6],\n",
      "        [5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([1, 2, 3])  # 1D tensor\n",
    "\n",
    "# Broadcasting in addition\n",
    "result = A + B\n",
    "print(\"Broadcasted Addition:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.5 Incompatible Shapes**\n",
    "If two tensors have incompatible shapes, an error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Example of incompatible shapes (will raise an error)\n",
    "try:\n",
    "    A = torch.tensor([[1, 2], [3, 4]])\n",
    "    B = torch.tensor([1, 2, 3])\n",
    "    print(A + B)  # This will fail\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Tensor Device Management (CPU vs. GPU)**\n",
    "\n",
    "PyTorch allows you to perform computations on both **CPU** and **GPU** for efficient deep learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1 Checking for GPU Availability**\n",
    "To check if a GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a GPU is available, it returns `\"cuda\"`, otherwise, it returns `\"cpu\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.2 Moving Tensors Between CPU and GPU**\n",
    "You can move tensors between CPU and GPU using `.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Device Before: cpu\n",
      "Tensor Device After: cpu\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor on CPU\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "print(\"Tensor Device Before:\", tensor.device)\n",
    "\n",
    "# Moving tensor to GPU (if available)\n",
    "tensor_gpu = tensor.to(device)\n",
    "print(\"Tensor Device After:\", tensor_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use:\n",
    "- `tensor.cuda()` to move to GPU\n",
    "- `tensor.cpu()` to move back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Moving to GPU\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tensor_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Moving back to CPU\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tensor_cpu \u001b[38;5;241m=\u001b[39m tensor_gpu\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32mc:\\Python\\Python3.10.11\\lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Moving to GPU\n",
    "tensor_gpu = tensor.cuda()\n",
    "\n",
    "# Moving back to CPU\n",
    "tensor_cpu = tensor_gpu.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3 Creating Tensors Directly on a Specific Device**\n",
    "You can create tensors directly on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on GPU: tensor([10, 20, 30])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor directly on GPU\n",
    "tensor_gpu = torch.tensor([10, 20, 30], device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Tensor on GPU:\", tensor_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can specify `device` while using tensor creation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Tensor Device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor_gpu = torch.zeros(3, device=device)\n",
    "print(\"Created Tensor Device:\", tensor_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.4 Performing Operations on GPU**\n",
    "Operations on GPU tensors are much faster than on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 5.9804582595825195\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Large tensor on CPU\n",
    "tensor_cpu = torch.randn(10000, 10000)\n",
    "\n",
    "# Large tensor on GPU (if available)\n",
    "tensor_gpu = tensor_cpu.to(device)\n",
    "\n",
    "# Timing CPU operation\n",
    "start = time.time()\n",
    "result_cpu = tensor_cpu @ tensor_cpu  # Matrix multiplication\n",
    "print(\"CPU Time:\", time.time() - start)\n",
    "\n",
    "# Timing GPU operation (if GPU is available)\n",
    "if torch.cuda.is_available():\n",
    "    start = time.time()\n",
    "    result_gpu = tensor_gpu @ tensor_gpu\n",
    "    torch.cuda.synchronize()  # Ensure GPU operations are completed\n",
    "    print(\"GPU Time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.5 Ensuring Tensors Are on the Same Device**\n",
    "Operations between tensors require them to be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CPU and GPU tensors\n",
    "tensor_cpu = torch.tensor([1, 2, 3])\n",
    "tensor_gpu = torch.tensor([4, 5, 6], device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Attempting an operation (this will raise an error if devices don't match)\n",
    "try:\n",
    "    result = tensor_cpu + tensor_gpu  # This will fail if one is on CPU and the other on GPU\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, move all tensors to the same device before operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "# Moving CPU tensor to GPU\n",
    "tensor_cpu = tensor_cpu.to(device)\n",
    "\n",
    "# Now the operation works\n",
    "result = tensor_cpu + tensor_gpu\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
