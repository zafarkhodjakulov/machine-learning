{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;color:#0F4C81;\">\n",
    "PyTorch Autograd Engine\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's **autograd** module enables automatic differentiation, making it essential for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1 Enabling Gradient Tracking**\n",
    "Tensors that require gradients must be created with `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([2.], requires_grad=True)\n",
      "Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor with gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(\"Tensor:\", x)\n",
    "print(\"Requires Grad:\", x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2 Computing Gradients**\n",
    "When performing operations on tensors with `requires_grad=True`, PyTorch tracks computations for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Output: tensor([4.], grad_fn=<PowBackward0>)\n",
      "Gradient (dy/dx): tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# Define a function y = x^2\n",
    "y = x ** 2\n",
    "print(\"Function Output:\", y)\n",
    "\n",
    "# Compute gradients (dy/dx)\n",
    "y.backward()\n",
    "\n",
    "# Gradient of y with respect to x\n",
    "print(\"Gradient (dy/dx):\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $y = x^2$, the derivative $dy/dx = 2x$. Given $x = 2$, we expect $dy/dx = 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3 Computing Gradients for Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-dimensional tensors, you must specify a gradient vector (usually ones for scalar sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (dy/dx): tensor([ 3., 12., 27.])\n"
     ]
    }
   ],
   "source": [
    "# Creating a vector with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Function: y = x^3\n",
    "y = x ** 3\n",
    "\n",
    "# Compute gradients using a gradient vector\n",
    "y.backward(torch.tensor([1.0, 1.0, 1.0]))  # Equivalent to sum(y).backward()\n",
    "\n",
    "# Gradients\n",
    "print(\"Gradient (dy/dx):\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $y = x^3$, the derivative is $dy/dx = 3x^2$, so at $x = [1,2,3]$, we expect $[3, 12, 27]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4 Stopping Gradient Computation**\n",
    "Sometimes, we want to disable gradient tracking to save memory and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Grad Computation: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(\"No Grad Computation:\", y.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `.detach()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detached Tensor Requires Grad: False\n"
     ]
    }
   ],
   "source": [
    "y = x.detach()\n",
    "print(\"Detached Tensor Requires Grad:\", y.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5 Zeroing Gradients**\n",
    "Gradients accumulate by default, so always reset them before a new computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Gradient: tensor([6.])\n",
      "Gradient After Zeroing: tensor([0.])\n",
      "Accumulated Gradient: tensor([6.])\n",
      "Gradient After Zeroing: tensor([0.])\n",
      "Accumulated Gradient: tensor([6.])\n",
      "Gradient After Zeroing: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "for _ in range(3):\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    print(\"Accumulated Gradient:\", x.grad)\n",
    "    \n",
    "    # Reset gradients\n",
    "    x.grad.zero_()\n",
    "    print(\"Gradient After Zeroing:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6 Higher-Order Gradients**\n",
    "PyTorch supports second-order differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Derivative (dy/dx): tensor([12.], grad_fn=<MulBackward0>)\n",
      "Second Derivative (d²y/dx²): tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Function: y = x^3\n",
    "y = x ** 3\n",
    "\n",
    "# First derivative: dy/dx\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "\n",
    "# Second derivative: d^2y/dx^2\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "\n",
    "print(\"First Derivative (dy/dx):\", grad1)\n",
    "print(\"Second Derivative (d²y/dx²):\", grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $y = x^3$, $dy/dx = 3x^2$ and $d^2y/dx^2 = 6x$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
