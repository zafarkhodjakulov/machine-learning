{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;color:#0F4C81;\">\n",
    "Backpropagation\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to PyTorch autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a = 1.0 \\\\\n",
    "b = 2.0 \\\\\n",
    "c = a^2 + \\sqrt{b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial a} = 2a = 2 \\cdot 1.0 = 2.0 \\\\\n",
    "\\frac{\\partial c}{\\partial b} = \\frac{1}{2\\sqrt{b}} = \\frac{1}{2\\cdot \\sqrt{2}} = 0.3536\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = a ** 2 + b ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None, b.grad=None\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}, {b.grad=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor(2.), b.grad=tensor(0.3536)\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}, {b.grad=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Chain rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x = 2.0 \\\\\n",
    "y = 3.0 \\\\\n",
    "z = x \\cdot y \\\\\n",
    "w = z^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = y = 3.0 \\\\\n",
    "\\frac{\\partial z}{\\partial y} = x = 2.0 \\\\ \n",
    "\\frac{\\partial w}{\\partial z} = 2z = 12.0 \\\\ \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w}{\\partial x} = \\frac{\\partial w}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x} = 12.0 \\cdot 3.0 = 36.0 \\\\\n",
    "\\frac{\\partial w}{\\partial y} = \\frac{\\partial w}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} = 12.0 \\cdot 2.0 = 24.0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None, y.grad=None\n",
      "x.grad=tensor(36.), y.grad=tensor(24.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = x * y\n",
    "z.retain_grad()\n",
    "w = z ** 2\n",
    "print(f'{x.grad=}, {y.grad=}')\n",
    "w.backward()\n",
    "print(f'{x.grad=}, {y.grad=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "X = torch.randn(3, 2)\n",
    "W = torch.randn(2, 1, requires_grad=True)\n",
    "mm = X @ W\n",
    "m = mm.mean()\n",
    "m.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839],\n",
       "        [ 0.0576]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How PyTorch Computes Gradients Automatically**\n",
    "PyTorch tracks operations on tensors that have `requires_grad=True` and builds a **dynamic computational graph**. During **backpropagation**, it uses the **chain rule** to compute gradients.\n",
    "\n",
    "### **Computational Graph in PyTorch**\n",
    "Whenever you perform operations on a tensor with `requires_grad=True`, PyTorch constructs a **computational graph**, where:\n",
    "- **Nodes** represent tensors.\n",
    "- **Edges** represent operations that transform tensors.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)  \n",
    "y = x ** 2  \n",
    "z = y + 3  \n",
    "z.backward()  \n",
    "\n",
    "print(x.grad)  # This prints 4.0 (since dz/dx = 2x = 2*2 = 4)\n",
    "```\n",
    "### **What Happens Here?**\n",
    "1. **Forward Pass:**\n",
    "   - $ y = x^2 $  \n",
    "   - $ z = y + 3 $\n",
    "2. **Backward Pass:**\n",
    "   - PyTorch computes **$ \\frac{dz}{dx} $** using the **chain rule**:  \n",
    "     $$\n",
    "     \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 1 \\times 2x = 2x\n",
    "     $$\n",
    "   - Since $ x = 2 $, we get $ x.grad = 4.0 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chain Rule & Backpropagation**\n",
    "PyTorch applies the **chain rule** recursively:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\n",
    "$$\n",
    "\n",
    "Example with multiple operations:\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = y + x\n",
    "w = 2 * z\n",
    "w.backward()  # Compute gradients\n",
    "\n",
    "print(x.grad)  # 14.0\n",
    "```\n",
    "**Gradient Calculation:**\n",
    "1. $ y = x^3 $, so $ \\frac{dy}{dx} = 3x^2 = 3(2^2) = 12 $\n",
    "2. $ z = y + x $, so $ \\frac{dz}{dx} = \\frac{dy}{dx} + 1 = 12 + 1 = 13 $\n",
    "3. $ w = 2z $, so $ \\frac{dw}{dz} = 2 $\n",
    "4. Using the chain rule:\n",
    "   $$\n",
    "   \\frac{dw}{dx} = 2 \\cdot 13 = 26\n",
    "   $$\n",
    "   \n",
    "So, `x.grad = 26`.\n",
    "\n",
    "---\n",
    "\n",
    "### **`backward()` & Computational Graph Cleanup**\n",
    "PyTorch automatically frees the computational graph after calling `.backward()` to **save memory**.  \n",
    "- To **retain** gradients for multiple backward passes, use `retain_graph=True`:\n",
    "  ```python\n",
    "  w.backward(retain_graph=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **`torch.no_grad()` for Disabling Gradients**\n",
    "If you don’t need gradients (e.g., during inference), use:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    y = x ** 2  # No gradient tracking\n",
    "```\n",
    "or:\n",
    "```python\n",
    "x.requires_grad_(False)  # Disables tracking for x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **`torch.autograd.grad()` for Manual Gradients**\n",
    "Instead of calling `.backward()`, you can compute gradients manually:\n",
    "```python\n",
    "grad = torch.autograd.grad(w, x)  # Computes dw/dx\n",
    "print(grad)  # (tensor(26.),)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- PyTorch builds a **computational graph** dynamically.  \n",
    "- Uses **backpropagation** & **chain rule** to compute gradients.  \n",
    "- `.backward()` propagates gradients automatically.  \n",
    "- Use `torch.no_grad()` for inference to save memory.  \n",
    "- Use `torch.autograd.grad()` for manual gradient computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building an Autograd Engine from Scratch**\n",
    "\n",
    "Let’s build a **basic autograd engine from scratch** similar to Andrej Karpathy's `micrograd`. This will help us understand how PyTorch’s **autograd** system works internally.\n",
    "\n",
    "We will create a minimal **automatic differentiation engine** that supports:\n",
    "- Scalar operations  \n",
    "- **Computational graphs**  \n",
    "- **Backward propagation** using the **chain rule**  \n",
    "\n",
    "\n",
    "## **Step 1: Define a Computational Graph Node**\n",
    "Each value in our computation graph needs to:\n",
    "- Store its **current value**.\n",
    "- Keep track of the **operation** that produced it.\n",
    "- Store **gradients** during backpropagation.\n",
    "\n",
    "We define a **Value class**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\"):\n",
    "        self.data = data  # Scalar value\n",
    "        self.grad = 0  # Gradient (initialized to 0)\n",
    "        self._backward = lambda: None  # Backward function\n",
    "        self._children = set(_children)  # Track parent nodes\n",
    "        self._op = _op  # Operation that produced this value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "```\n",
    "\n",
    "\n",
    "## **Step 2: Implement Basic Operations**\n",
    "We define **addition, multiplication, and power** with proper **gradient computation**.\n",
    "\n",
    "### **Addition**\n",
    "For **$ z = x + y $**, we apply:\n",
    "$$\n",
    "\\frac{dz}{dx} = 1, \\quad \\frac{dz}{dy} = 1\n",
    "$$\n",
    "```python\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)  # Support numbers\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad  # dz/dx = 1\n",
    "            other.grad += out.grad  # dz/dy = 1\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "```\n",
    "\n",
    "\n",
    "### **Multiplication**\n",
    "For **$ z = x \\cdot y $**, we apply:\n",
    "$$\n",
    "\\frac{dz}{dx} = y, \\quad \\frac{dz}{dy} = x\n",
    "$$\n",
    "```python\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad  # dz/dx = y\n",
    "            other.grad += self.data * out.grad  # dz/dy = x\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "```\n",
    "\n",
    "\n",
    "### **Power Function**\n",
    "For **$ z = x^n $**, we apply:\n",
    "$$\n",
    "\\frac{dz}{dx} = n \\cdot x^{n-1}\n",
    "$$\n",
    "```python\n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"Only supports int or float exponent\"\n",
    "        out = Value(self.data ** exponent, (self,), f\"**{exponent}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += exponent * (self.data ** (exponent - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Implement `backward()` for Backpropagation**\n",
    "To compute gradients, we use **reverse-mode autodiff**:\n",
    "- Start from the **final node** (i.e., loss).\n",
    "- Recursively apply **the chain rule** using `_backward()`.\n",
    "- Traverse nodes **in topological order**.\n",
    "\n",
    "```python\n",
    "    def backward(self):\n",
    "        # Topological sorting using DFS\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)  # Append after children\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        # Initialize gradient at the final node (output)\n",
    "        self.grad = 1\n",
    "\n",
    "        # Traverse in reverse order and apply chain rule\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "```\n",
    "\n",
    "\n",
    "## **Step 4: Test the Autograd Engine**\n",
    "Now, let's test our **manual autograd engine** by computing derivatives.\n",
    "\n",
    "### **Example: Compute $ y = x^2 + 2x + 1 $ and its derivative**\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x + 2\n",
    "$$\n",
    "\n",
    "```python\n",
    "x = Value(3.0)  # Initialize variable\n",
    "y = x**2 + 2*x + 1  # Compute function\n",
    "y.backward()  # Compute gradients\n",
    "\n",
    "print(f\"x: {x.data}, y: {y.data}, dy/dx: {x.grad}\")\n",
    "```\n",
    "#### **Expected Output:**\n",
    "```\n",
    "x: 3.0, y: 16.0, dy/dx: 8.0\n",
    "```\n",
    "since:\n",
    "$$\n",
    "\\frac{dy}{dx} = 2(3) + 2 = 8\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
