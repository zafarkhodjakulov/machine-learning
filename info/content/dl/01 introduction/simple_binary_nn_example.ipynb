{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **x₁**  | **x₂**  | **y**  |\n",
    "|--------|--------|-------|\n",
    "| 0.1    | 0.7    | 1     |\n",
    "| 0.4    | 0.9    | 1     |\n",
    "| 0.3    | 0.2    | 0     |\n",
    "| 0.8    | 0.4    | 0     |\n",
    "| 0.9    | 0.8    | 1     |\n",
    "| 0.2    | 0.1    | 0     |\n",
    "| 0.5    | 0.5    | 1     |\n",
    "| 0.6    | 0.3    | 0     |\n",
    "| 0.7    | 0.9    | 1     |\n",
    "| 0.2    | 0.8    | 1     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a simple neural network manually is a great exercise to understand how neural networks work. Here's a step-by-step process to train a simple neural network on the given dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Define the Neural Network Structure**\n",
    "- **Input layer**: 2 inputs (`x1` and `x2`).\n",
    "- **Hidden layer**: 2 neurons (for simplicity) with sigmoid activation.\n",
    "- **Output layer**: 1 neuron (since `y` is binary) with sigmoid activation.\n",
    "\n",
    "\n",
    "### **Example Calculation (1 Epoch, 1 Sample)**\n",
    "1. **Given** $ x_1 = 1, x_2 = 2, y = 0 $\n",
    "2. **Feedforward** (compute $ z_1, z_2, h_1, h_2, z_3, \\hat{y} $)\n",
    "3. **Loss**: Calculate cross-entropy loss.\n",
    "4. **Backpropagate**: Calculate $ \\delta_3, \\delta_1, \\delta_2 $ and update weights and biases.\n",
    "\n",
    "<a href=\"https://lucid.app/lucidchart/f1747941-87bb-470d-9e7d-99eb4992e12b/edit?beaconFlowId=F6CBE2C75F2F7444&invitationId=inv_38519930-d244-4e91-b32d-d154cfd90e9f&page=0_0#\">Neural Network Chart</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "data = [\n",
    "    (0.1, 0.7, 1), # first row\n",
    "    (0.4, 0.9, 1),\n",
    "    (0.3, 0.2, 0),\n",
    "    (0.8, 0.4, 0),\n",
    "    (0.9, 0.8, 1),\n",
    "    (0.2, 0.1, 0),\n",
    "    (0.5, 0.5, 1),\n",
    "    (0.6, 0.3, 0),\n",
    "    (0.7, 0.9, 1),\n",
    "    (0.2, 0.8, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Initialize Weights and Biases**\n",
    "- **Weights**: Randomly initialize weights for each connection. \n",
    "  - Between input and hidden layer: $ w_1, w_2, w_3, w_4 $ (for 2 neurons).\n",
    "  - Between hidden layer and output layer: $ w_5, w_6 $ (for 1 neuron).\n",
    "\n",
    "- **Biases**: Randomly initialize biases for each layer.\n",
    "  - Hidden layer biases: $ b_1, b_2 $ for 2 neurons.\n",
    "  - Output layer bias: $ b_3 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1=-7.16, w2=-9.21, w3=-7.46, w4=-9.45\n",
      "b1=-9.78, b2=-6.35\n",
      "w5=-5.07, w6=-6.58\n",
      "b3=-6.76\n"
     ]
    }
   ],
   "source": [
    "w1 = round(5*random.random() - 10, 2)\n",
    "w2 = round(5*random.random() - 10, 2)\n",
    "w3 = round(5*random.random() - 10, 2)\n",
    "w4 = round(5*random.random() - 10, 2)\n",
    "\n",
    "b1 = round(5*random.random() - 10, 2)\n",
    "b2 = round(5*random.random() - 10, 2)\n",
    "\n",
    "w5 = round(5*random.random() - 10, 2)\n",
    "w6 = round(5*random.random() - 10, 2)\n",
    "\n",
    "b3 = round(5*random.random() - 10, 2)\n",
    "\n",
    "print(f\"{w1=}, {w2=}, {w3=}, {w4=}\")\n",
    "print(f\"{b1=}, {b2=}\")\n",
    "print(f\"{w5=}, {w6=}\")\n",
    "print(f\"{b3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Forward Pass (Feedforward)**\n",
    "1. **Input to Hidden Layer**\n",
    "   - Calculate the weighted sum $ z_1 $ and $ z_2 $ for each hidden neuron:\n",
    "     $$\n",
    "     z_1 = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b_1\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     z_2 = w_3 \\cdot x_1 + w_4 \\cdot x_2 + b_2\n",
    "     $$\n",
    "   - Apply the sigmoid activation function to each neuron in the hidden layer:\n",
    "     $$\n",
    "     h_1 = \\frac{1}{1 + e^{-z_1}}, \\quad h_2 = \\frac{1}{1 + e^{-z_2}}\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=0.1, x2=0.7, y=1\n"
     ]
    }
   ],
   "source": [
    "x1, x2, y = data[0]\n",
    "print(f\"{x1=}, {x2=}, {y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1=-16.942999999999998, z2=-13.710999999999999\n",
      "h1=4.382768928580775e-08, h2=1.1101658824394257e-06\n"
     ]
    }
   ],
   "source": [
    "z1 = w1*x1 + w2*x2 + b1\n",
    "z2 = w3*x1 + w4*x2 + b2\n",
    "print(f\"{z1=}, {z2=}\")\n",
    "\n",
    "h1 = 1/(1 + math.exp(-z1))\n",
    "h2 = 1/(1 + math.exp(-z2))\n",
    "print(f\"{h1=}, {h2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Hidden Layer to Output Layer**\n",
    "   - Calculate the weighted sum for the output neuron:\n",
    "     $$\n",
    "     z_3 = w_5 \\cdot h_1 + w_6 \\cdot h_2 + b_3\n",
    "     $$\n",
    "   - Apply the sigmoid activation function to get the final output $ \\hat{y} $:\n",
    "     $$\n",
    "     \\hat{y} = \\frac{1}{1 + e^{-z_3}}\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z3=-6.7600075270978905\n",
      "y_pred=0.001157878212205724\n"
     ]
    }
   ],
   "source": [
    "z3 = w5 * h1 + w6 * h2 + b3\n",
    "print(f\"{z3=}\")\n",
    "\n",
    "y_pred = 1/(1 + math.exp(-z3))\n",
    "print(f\"{y_pred=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Compute the Loss**\n",
    "- Use the binary cross-entropy loss (since $ y $ is binary):\n",
    "  $$\n",
    "  L = - \\left( y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right)\n",
    "  $$\n",
    "  This tells us how far the prediction $ \\hat{y} $ is from the actual $ y $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=6.761166076168972\n"
     ]
    }
   ],
   "source": [
    "L = -(y * math.log(y_pred) + (1-y)*math.log(1-y_pred))\n",
    "print(f\"{L=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Backward Pass (Backpropagation)**\n",
    "1. **Error for Output Neuron**\n",
    "   - Calculate the derivative of the loss $ L $ with respect to $ z_3 $ (output neuron's pre-activation):\n",
    "     $$\n",
    "     \\delta_3 = \\hat{y} - y\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta3=-0.9988421217877943\n"
     ]
    }
   ],
   "source": [
    "delta3 = y_pred - y\n",
    "print(f\"{delta3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Update Weights from Hidden to Output**\n",
    "   - Calculate gradients for $ w_5 $, $ w_6 $, and $ b_3 $:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_5} = \\delta_3 \\cdot h_1\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_6} = \\delta_3 \\cdot h_2\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b_3} = \\delta_3\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w5=-4.377694215929239e-08\n",
      "grad_w6=-1.108880445552215e-06\n",
      "grad_b3=-0.9988421217877943\n"
     ]
    }
   ],
   "source": [
    "grad_w5 = delta3 * h1\n",
    "grad_w6 = delta3 * h2\n",
    "grad_b3 = delta3\n",
    "\n",
    "print(f\"{grad_w5=}\")\n",
    "print(f\"{grad_w6=}\")\n",
    "print(f\"{grad_b3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Error for Hidden Layer Neurons**\n",
    "   - Backpropagate the error from the output layer to the hidden layer using the chain rule:\n",
    "     $$\n",
    "     \\delta_1 = \\delta_3 \\cdot w_5 \\cdot h_1 \\cdot (1 - h_1)\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\delta_2 = \\delta_3 \\cdot w_6 \\cdot h_2 \\cdot (1 - h_2)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta1=2.2194908702009636e-07\n",
      "delta2=7.296425231482227e-06\n"
     ]
    }
   ],
   "source": [
    "delta1 = delta3 * w5 * h1 * (1 - h1)\n",
    "delta2 = delta3 * w6 * h2 * (1 - h2)\n",
    "print(f\"{delta1=}\")\n",
    "print(f\"{delta2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Update Weights from Input to Hidden**\n",
    "   - Calculate gradients for $ w_1, w_2, w_3, w_4 $, and biases $ b_1, b_2 $:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_1} = \\delta_1 \\cdot x_1\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_2} = \\delta_1 \\cdot x_2\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_3} = \\delta_2 \\cdot x_1\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w_4} = \\delta_2 \\cdot x_2\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b_1} = \\delta_1\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b_2} = \\delta_2\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w1=2.2194908702009636e-08\n",
      "grad_w2=1.5536436091406745e-07\n",
      "grad_w3=7.296425231482227e-07\n",
      "grad_w4=5.107497662037558e-06\n",
      "grad_b1=2.2194908702009636e-07\n",
      "grad_b2=7.296425231482227e-06\n"
     ]
    }
   ],
   "source": [
    "grad_w1 = delta1 * x1\n",
    "grad_w2 = delta1 * x2\n",
    "grad_w3 = delta2 * x1\n",
    "grad_w4 = delta2 * x2\n",
    "grad_b1 = delta1\n",
    "grad_b2 = delta2\n",
    "\n",
    "print(f\"{grad_w1=}\")\n",
    "print(f\"{grad_w2=}\")\n",
    "print(f\"{grad_w3=}\")\n",
    "print(f\"{grad_w4=}\")\n",
    "print(f\"{grad_b1=}\")\n",
    "print(f\"{grad_b2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Update Weights and Biases**\n",
    "- Update each parameter using gradient descent:\n",
    "  $$\n",
    "  w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "  $$\n",
    "  $$\n",
    "  b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
    "  $$\n",
    "  Here, $ \\eta $ is the learning rate (like 0.1 or 0.01). Apply this to all weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1=-7.160000000221949\n",
      "w2=-9.210000001553645\n",
      "w3=-7.460000007296425\n",
      "w4=-9.450000051074976\n",
      "w5=-5.0699999995622305\n",
      "w6=-6.5799999889111955\n",
      "b1=-9.78000000221949\n",
      "b2=-6.350000072964252\n",
      "b3=-6.7500115787821215\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 # learning rate\n",
    "\n",
    "w1 -= lr*grad_w1\n",
    "w2 -= lr*grad_w2\n",
    "w3 -= lr*grad_w3\n",
    "w4 -= lr*grad_w4\n",
    "w5 -= lr*grad_w5\n",
    "w6 -= lr*grad_w6\n",
    "b1 -= lr*grad_b1\n",
    "b2 -= lr*grad_b2\n",
    "b3 -= lr*grad_b3\n",
    "\n",
    "print(f\"{w1=}\")\n",
    "print(f\"{w2=}\")\n",
    "print(f\"{w3=}\")\n",
    "print(f\"{w4=}\")\n",
    "print(f\"{w5=}\")\n",
    "print(f\"{w6=}\")\n",
    "print(f\"{b1=}\")\n",
    "print(f\"{b2=}\")\n",
    "print(f\"{b3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Repeat for Each Training Sample**\n",
    "- For each sample, do a forward pass, calculate the loss, perform backpropagation, and update the weights.\n",
    "- Repeat for multiple epochs (full passes over all the samples) until the loss is small or the weights stop changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Steps**\n",
    "1. **Initialize** weights and biases.\n",
    "2. **Feedforward** to calculate $ \\hat{y} $.\n",
    "3. **Calculate loss** between $ \\hat{y} $ and true $ y $.\n",
    "4. **Backpropagate** errors to update weights and biases.\n",
    "5. **Repeat** for all 10 samples for several epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example Calculation (1 Epoch, All Samples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE 1\n",
      "y: 1\n",
      "y_pred: 0.0013992735716420975\n",
      "Loss: 6.5718020544228875\n",
      "w1=-7.160000004354449\n",
      "w2=-9.21000002396348\n",
      "w3=-7.460000138691741\n",
      "w4=-9.45000078215713\n",
      "w5=-5.069999993363855\n",
      "w6=-6.579999833870503\n",
      "b1=-9.780000033645265\n",
      "b2=-6.350001093137008\n",
      "b3=-6.470534201217211\n",
      "====================================================================================================\n",
      "SAMPLE 2\n",
      "y: 1\n",
      "y_pred: 0.001546004336745643\n",
      "Loss: 6.472081523681357\n",
      "w1=-7.160000004518626\n",
      "w2=-9.210000024332874\n",
      "w3=-7.460000143393213\n",
      "w4=-9.450000792735441\n",
      "w5=-5.0699999932829005\n",
      "w6=-6.57999983208423\n",
      "b1=-9.780000034055705\n",
      "b2=-6.350001104890688\n",
      "b3=-6.370688801650886\n",
      "====================================================================================================\n",
      "SAMPLE 3\n",
      "y: 0\n",
      "y_pred: 0.0017077330063818932\n",
      "Loss: 0.0017091928446385373\n",
      "w1=-7.160000004246785\n",
      "w2=-9.210000024151647\n",
      "w3=-7.460000133904561\n",
      "w4=-9.450000786409673\n",
      "w5=-5.069999993461625\n",
      "w6=-6.579999836891179\n",
      "b1=-9.780000033149571\n",
      "b2=-6.350001073261849\n",
      "b3=-6.370859574951524\n",
      "====================================================================================================\n",
      "SAMPLE 4\n",
      "y: 0\n",
      "y_pred: 0.00170776548802987\n",
      "Loss: 0.0017092253818518656\n",
      "w1=-7.1600000042435825\n",
      "w2=-9.210000024150045\n",
      "w3=-7.46000013381284\n",
      "w4=-9.450000786363812\n",
      "w5=-5.069999993462415\n",
      "w6=-6.579999836908604\n",
      "b1=-9.780000033145567\n",
      "b2=-6.350001073147197\n",
      "b3=-6.371030351500327\n",
      "====================================================================================================\n",
      "SAMPLE 5\n",
      "y: 1\n",
      "y_pred: 0.0017074755360213078\n",
      "Loss: 6.372739293988697\n",
      "w1=-7.160000004269443\n",
      "w2=-9.21000002417303\n",
      "w3=-7.460000134465724\n",
      "w4=-9.450000786944154\n",
      "w5=-5.069999993456747\n",
      "w6=-6.579999836798357\n",
      "b1=-9.780000033174302\n",
      "b2=-6.350001073872624\n",
      "b3=-6.271201099053929\n",
      "====================================================================================================\n",
      "SAMPLE 6\n",
      "y: 0\n",
      "y_pred: 0.0018844500611097457\n",
      "Loss: 0.001886227870939771\n",
      "w1=-7.160000003241609\n",
      "w2=-9.210000023659113\n",
      "w3=-7.460000096607367\n",
      "w4=-9.450000768014975\n",
      "w5=-5.069999994470396\n",
      "w6=-6.579999865570499\n",
      "b1=-9.780000028035133\n",
      "b2=-6.350000884580841\n",
      "b3=-6.27138954406004\n",
      "====================================================================================================\n",
      "SAMPLE 7\n",
      "y: 1\n",
      "y_pred: 0.0018860324607269544\n",
      "Loss: 6.273279883504906\n",
      "w1=-7.160000007232388\n",
      "w2=-9.210000027649892\n",
      "w3=-7.460000218687593\n",
      "w4=-9.4500008900952\n",
      "w5=-5.069999992896124\n",
      "w6=-6.579999828464032\n",
      "b1=-9.780000036016691\n",
      "b2=-6.350001128741292\n",
      "b3=-6.171578147306112\n",
      "====================================================================================================\n",
      "SAMPLE 8\n",
      "y: 0\n",
      "y_pred: 0.002083571441793392\n",
      "Loss: 0.002085745096604803\n",
      "w1=-7.160000007201565\n",
      "w2=-9.21000002763448\n",
      "w3=-7.460000217727559\n",
      "w4=-9.450000889615183\n",
      "w5=-5.069999992906256\n",
      "w6=-6.579999828707202\n",
      "b1=-9.780000035965319\n",
      "b2=-6.350001127141234\n",
      "b3=-6.171786504450292\n",
      "====================================================================================================\n",
      "SAMPLE 9\n",
      "y: 1\n",
      "y_pred: 0.0020831547137327515\n",
      "Loss: 6.173871844985878\n",
      "w1=-7.16000000723508\n",
      "w2=-9.210000027677571\n",
      "w3=-7.460000218604735\n",
      "w4=-9.45000089074298\n",
      "w5=-5.069999992896812\n",
      "w6=-6.57999982851676\n",
      "b1=-9.780000036013199\n",
      "b2=-6.350001128394344\n",
      "b3=-6.071994819921666\n",
      "====================================================================================================\n",
      "SAMPLE 10\n",
      "y: 1\n",
      "y_pred: 0.002301256230701545\n",
      "Loss: 6.074300117891464\n",
      "w1=-7.160000008097753\n",
      "w2=-9.21000003112826\n",
      "w3=-7.460000245473685\n",
      "w4=-9.450000998218782\n",
      "w5=-5.06999999204605\n",
      "w6=-6.57999980809962\n",
      "b1=-9.780000040326561\n",
      "b2=-6.350001262739094\n",
      "b3=-5.972224945544736\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "w1=-7.160000008097753\n",
      "w2=-9.21000003112826\n",
      "w3=-7.460000245473685\n",
      "w4=-9.450000998218782\n",
      "w5=-5.06999999204605\n",
      "w6=-6.57999980809962\n",
      "b1=-9.780000040326561\n",
      "b2=-6.350001262739094\n",
      "b3=-5.972224945544736\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 # learning rate\n",
    "i = 1\n",
    "for x1, x2, y in data:\n",
    "    print(f\"SAMPLE {i}\")\n",
    "    z1 = w1*x1 + w2*x2 + b1\n",
    "    z2 = w3*x1 + w4*x2 + b2\n",
    "\n",
    "    h1 = 1/(1 + math.exp(-z1))\n",
    "    h2 = 1/(1 + math.exp(-z2))\n",
    "\n",
    "    z3 = w5 * h1 + w6 * h2 + b3\n",
    "\n",
    "    y_pred = 1/(1 + math.exp(-z3))\n",
    "    print(f\"y: {y}\")\n",
    "    print(f\"y_pred: {y_pred}\")\n",
    "\n",
    "    L = -(y * math.log(y_pred) + (1-y)*math.log(1-y_pred))\n",
    "    print(f\"Loss: {L}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    delta3 = y_pred - y\n",
    "\n",
    "    grad_w5 = delta3 * h1\n",
    "    grad_w6 = delta3 * h2\n",
    "    grad_b3 = delta3\n",
    "\n",
    "\n",
    "    delta1 = delta3 * w5 * h1 * (1 - h1)\n",
    "    delta2 = delta3 * w6 * h2 * (1 - h2)\n",
    "\n",
    "    grad_w1 = delta1 * x1\n",
    "    grad_w2 = delta1 * x2\n",
    "    grad_w3 = delta2 * x1\n",
    "    grad_w4 = delta2 * x2\n",
    "    grad_b1 = delta1\n",
    "    grad_b2 = delta2\n",
    "\n",
    "    w1 -= lr*grad_w1\n",
    "    w2 -= lr*grad_w2\n",
    "    w3 -= lr*grad_w3\n",
    "    w4 -= lr*grad_w4\n",
    "    w5 -= lr*grad_w5\n",
    "    w6 -= lr*grad_w6\n",
    "    b1 -= lr*grad_b1\n",
    "    b2 -= lr*grad_b2\n",
    "    b3 -= lr*grad_b3\n",
    "\n",
    "    print(f\"{w1=}\")\n",
    "    print(f\"{w2=}\")\n",
    "    print(f\"{w3=}\")\n",
    "    print(f\"{w4=}\")\n",
    "    print(f\"{w5=}\")\n",
    "    print(f\"{w6=}\")\n",
    "    print(f\"{b1=}\")\n",
    "    print(f\"{b2=}\")\n",
    "    print(f\"{b3=}\")\n",
    "\n",
    "    i+=1\n",
    "    print(\"=\"*100)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"{w1=}\")\n",
    "print(f\"{w2=}\")\n",
    "print(f\"{w3=}\")\n",
    "print(f\"{w4=}\")\n",
    "print(f\"{w5=}\")\n",
    "print(f\"{w6=}\")\n",
    "print(f\"{b1=}\")\n",
    "print(f\"{b2=}\")\n",
    "print(f\"{b3=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
