{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dd825e",
   "metadata": {},
   "source": [
    "- What is convolution\n",
    "- What is pooling\n",
    "- What is subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a44ab",
   "metadata": {},
   "source": [
    "Convolution is a mathematical operation that combines two functions to describe the overlap between them. Convolution takes two functions and “slides” one of them over the other, multiplying the function values at each point where they overlap, and adding up the products to create a new function.\n",
    "\n",
    "Formally, convolution is an integral that expresses the amount of overlap of one function, $f(t)$,  as it is shifted over function $g(t)$, expressed as:\n",
    "$$\n",
    "(f*g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c606e",
   "metadata": {},
   "source": [
    "<img src=\"images/convolution_animated.gif\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69a3ab",
   "metadata": {},
   "source": [
    "In image processing, convolutional filtering can be used to implement algorithms such as edge detection, image sharpening, and image blurring.\n",
    "\n",
    "This is done by selecting the appropriate kernel (convolution matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee811d1",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/image_convolution_animated.gif\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f9c44",
   "metadata": {},
   "source": [
    "Convolution plays a key role in convolutional neural networks (CNNs). CNNs are a type of deep network commonly used to analyze images. CNNs eliminate the need for manual feature extraction, which is why they work very well for complex problems such as image classification and medical image analysis. CNNs are effective for non-image data analysis such as audio, time-series, and signal data.\n",
    "\n",
    "To be more specific, similar to an actual biological neural network, CNN could\n",
    "identify the fraction of the image and recognize the unique feature which does not\n",
    "alter even if certain transformations such as shifting, scaling, and rotating. The usual CNN consists of the following layers, Convolution layer, ReLU\n",
    "layer, Pooling layer, and FC (Full Connection) layer. The convolutional layer is used\n",
    "to extract the main features through operations. The pooling layer is very effective in\n",
    "reducing the size of the matrix, thus increasing efficiency. Compared with other tools\n",
    "of image recognition, with its own known pattern and following learning, there is no\n",
    "necessity to input detailed and complex mathematical arithmetic expressions for the\n",
    "computer to judge and CNN could come into forming specific mapping capability\n",
    "for further operations of detecting images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85e513",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/general_convolutional_neural_network.png\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c499a",
   "metadata": {},
   "source": [
    "A fully connected neural network consists of a series of fully connected layers,\n",
    "that connect every neuron in one layer to every neuron in the other layer. The\n",
    "main problem with fully connected neural networks are that the number of weights\n",
    "required is very large for certain types of data. For example, an image of 224×224×3 would require 150,528 weights in just the first hidden layer, and will grow\n",
    "quickly for even bigger images. You can imagine how computationally intensive\n",
    "things would become once the images reach dimensions as large as 8K resolution\n",
    "images (7680×4320), training such a network would require a lot of time and\n",
    "resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45e13b",
   "metadata": {},
   "source": [
    "However for image data, repeating patterns can occur in different places. Hence\n",
    "we can train many smaller detectors, capable of sliding across an image, to take\n",
    "advantage of the repeating patterns. This would reduce the\n",
    "number of weights required to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a54ba",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/pattern_detection_in_cnn.png\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9aab8",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/beak_detector.png\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d14f7c",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network is a neural network with some convolutional\n",
    "layers (and some other layers). A convolutional layer has a number of filters that\n",
    "does the convolutional operation.\n",
    "\n",
    "The convolution operation is very similar to image processing\n",
    "filters such as the Sobel filter and Gaussian Filter. The Kernel slides across an image and multiplies the weights with each aligned pixel, element-wise across the filter.\n",
    "Afterwards the bias value is added to the output.\n",
    "\n",
    "There are three hyperparameters deciding the spatial of the output feature map:\n",
    "\n",
    "- Stride (S) is the step each time we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n",
    "\n",
    "- Padding (P): The inputs will be padded with a border of size according to the value specified. Most commonly, zero-padding is used to pad these locations. In neural network frameworks (caffe, tensorflow, pytorch, mxnet), the size of this zero-padding is a hyperparameter. The size of zero-padding can also be used to control the spatial size of the output volumes.\n",
    "\n",
    "- Depth (D): The depth of the output volume is a hyperparameter too, it corresponds to the number of filters we use for a convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc5312",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/convolution_operation.png\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d9899",
   "metadata": {},
   "source": [
    "Given $w$ as the width of input, and $F$ is the width of the filter, with $P$ and $S$ as\n",
    "padding and stride respectively, the output width will be: $\\frac{W + 2P − F}{S}+1$. Generally, set $P = \\frac{F − 1}{2}$\n",
    "when the stride is $S = 1$ ensures that the input volume and output volume will have\n",
    "the same size spatially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15026242",
   "metadata": {},
   "source": [
    "For an input of 7×7×3 and a output depth of 2, we will have 6 kernels as shown\n",
    "below. 3 for the first depth output and another 3 for the second depth output. The\n",
    "outputs of each filter is summed up to generate the output feature map.\n",
    "In this example, the output from each Kernel of Filter W1 is as\n",
    "follows:\n",
    "<pre>\n",
    "Output of Kernel 1 = 1\n",
    "Output of Kernel 2 = −2 \n",
    "Output of Kernel 3 = 2 \n",
    "Output of Filter W1 = Output of Kernel 1 + Output of Kernel 2 + Output of Kernel 3 + bias \n",
    "    = 1 − 2 + 2 + 0 = 1.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acb799",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/convolution_example.png\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b37332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8db693ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "c = nn.Conv2d(3, 2, kernel_size=4)\n",
    "c.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ac63409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9037,  0.5184,  0.0768,  0.9506],\n",
       "          [ 0.3873,  0.5276,  0.4582,  0.3262],\n",
       "          [-0.0666,  0.2944, -0.2848,  0.7257],\n",
       "          [ 0.5757, -0.0773,  0.4156,  0.3511]],\n",
       "\n",
       "         [[-1.2760, -0.1103, -0.0280,  0.4482],\n",
       "          [-0.8313,  0.2466, -0.5139, -0.5076],\n",
       "          [ 0.8714,  0.2920, -0.3182, -0.0161],\n",
       "          [-0.2489, -0.6562, -0.3168, -0.4677]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 7, 7)\n",
    "c(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03d4f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.9037\t  0.5184\t  0.0768\t  0.9506\t\n",
      "  0.3873\t  0.5276\t  0.4582\t  0.3262\t\n",
      " -0.0666\t  0.2944\t -0.2848\t  0.7257\t\n",
      "  0.5757\t -0.0773\t  0.4156\t  0.3511\t\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(\n",
    "            f\"{(\n",
    "                (x[0, 0][i:4+i, j:4+j] * c.weight[0, 0]).sum() +\n",
    "                (x[0, 1][i:4+i, j:4+j] * c.weight[0, 1]).sum() +\n",
    "                (x[0, 2][i:4+i, j:4+j] * c.weight[0, 2]).sum() + \n",
    "                c.bias[0]).item():.4f}\".rjust(8),\n",
    "            end='\\t'\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3171005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.9037\t  0.5184\t  0.0768\t  0.9506\t\n",
      "  0.3873\t  0.5276\t  0.4582\t  0.3262\t\n",
      " -0.0666\t  0.2944\t -0.2848\t  0.7257\t\n",
      "  0.5757\t -0.0773\t  0.4156\t  0.3511\t\n",
      "------------------------------------------------------------\n",
      " -1.2760\t -0.1103\t -0.0280\t  0.4482\t\n",
      " -0.8313\t  0.2466\t -0.5139\t -0.5076\t\n",
      "  0.8714\t  0.2920\t -0.3182\t -0.0161\t\n",
      " -0.2489\t -0.6562\t -0.3168\t -0.4677\t\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for out in range(2):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            print(\n",
    "                f\"{(\n",
    "                    (x[0, 0][i:4+i, j:4+j] * c.weight[out, 0]).sum() +\n",
    "                    (x[0, 1][i:4+i, j:4+j] * c.weight[out, 1]).sum() +\n",
    "                    (x[0, 2][i:4+i, j:4+j] * c.weight[out, 2]).sum() + \n",
    "                    c.bias[out]).item():.4f}\".rjust(8),\n",
    "                end='\\t'\n",
    "            )\n",
    "        print()\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63c340",
   "metadata": {},
   "source": [
    "**Pooling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5012d",
   "metadata": {},
   "source": [
    "Pooling layer is used in CNNs to reduce the spatial dimensions (width and height) of the input feature maps while retaining the most important information. It involves sliding a two-dimensional filter over each channel of a feature map and summarizing the features within the region covered by the filter.\n",
    "A pooling layer helps reduce computation time and\n",
    "gradually build up spatial and configural invariance. For image understanding,\n",
    "pooling layer helps extract more semantic meaning. The max pooling layer simply\n",
    "returns the maximum value over the values that the kernel operation is applied on.\n",
    "The example below illustrates the outputs of a max pooling and average\n",
    "pooling operation respectively, given a kernel of size 2 and stride 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d9867",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/pooling.png\" width=\"500\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abf580",
   "metadata": {},
   "source": [
    "**Flattening**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8ae31",
   "metadata": {},
   "source": [
    "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear\n",
    "combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in\n",
    "that space.\n",
    "\n",
    "By flattening the image into a column vector, we have converted our input image\n",
    "into a suitable form for our Multi-Level Perceptron. The flattened output is fed\n",
    "to a feed-forward neural network and backpropagation applied to every iteration\n",
    "of training. Over a series of epochs, the model is able to distinguish between\n",
    "dominating and certain low-level features in images and classify them using the\n",
    "Softmax Classification technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52060c43",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/flattening.png\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cf90e",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "https://medium.com/@siddheshb008/understanding-convolution-neural-networks-a30211e12a06\n",
    "https://medium.com/@siddheshb008/understanding-convolutional-neural-networks-part-2-98694dd47923"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4db37e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
