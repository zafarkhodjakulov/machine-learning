{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1518f87",
   "metadata": {},
   "source": [
    "**LeNet 5** architecture is the \"Hello World\" in the domain of Convolution Neural Networks. The backpropagation rule was first applied to all reasonable applications in 1989 by Yann LeCun and colleagues at Bell Labs.\n",
    "\n",
    "LeNet is a convolutional neural network that Yann LeCun introduced in 1989. LeNet is a common term for LeNet-5, a simple convolutional neural network.\n",
    "\n",
    "The LeNet-5 signifies CNN’s emergence and outlines its core components. However, it was not popular at the time due to a lack of hardware, especially GPU (Graphics Process Unit, a specialised electronic circuit designed to change memory to accelerate the creation of images during a buffer intended for output to a show device) and alternative algorithms, like SVM, which could perform effects similar to or even better than those of the LeNet.\n",
    "\n",
    "**Features of LeNet-5**:\n",
    "- Every convolutional layer includes three parts: convolution, pooling, and nonlinear activation functions\n",
    "- Using convolution to extract spatial features (Convolution was called receptive fields originally)\n",
    "- The average pooling layer is used for subsampling.\n",
    "- ‘tanh’ is used as the activation function\n",
    "- Using Multi-Layered Perceptron or Fully Connected Layers as the last classifier\n",
    "- The sparse connection between layers reduces the complexity of computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e858d3b",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235af91f",
   "metadata": {},
   "source": [
    "The LeNet-5 CNN architecture has seven layers. Three convolutional layers, two subsampling layers, and two fully linked layers make up the layer composition.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/lenet-5 architecture.webp\" width=\"900\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdac3c",
   "metadata": {},
   "source": [
    "**First Layer**\n",
    "\n",
    "A 32x32 grayscale image serves as the input for LeNet-5 and is processed by the first convolutional layer comprising six feature maps or filters with a stride of one. From 32x32x1 to 28x28x6, the image’s dimensions shift.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/first_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f78cf4",
   "metadata": {},
   "source": [
    "**Second Layer**\n",
    "\n",
    "Then, using a filter size of 22 and a stride of 2, the LeNet-5 adds an average pooling layer or sub-sampling layer. 14x14x6 will be the final image’s reduced size.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/second_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f26be2",
   "metadata": {},
   "source": [
    "**Third Layer**\n",
    "\n",
    "A second convolutional layer with 16 feature maps of size 55 and a stride of 1 is then present. Only 10 of the 16 feature maps in this layer are linked to the six feature maps in the layer below, as can be seen in the illustration below.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/third_layer.webp\" width=\"700\" />\n",
    "</div>\n",
    "\n",
    "The primary goal is to disrupt the network’s symmetry while maintaining a manageable number of connections. Because of this, there are 1516 training parameters instead of 2400 in these layers, and similarly, there are 151600 connections instead of 240000.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/third_layer2.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d46d4",
   "metadata": {},
   "source": [
    "**Fourth Layer**\n",
    "\n",
    "With a filter size of 22 and a stride of 2, the fourth layer (S4) is once more an average pooling layer. The output will be decreased to 5x5x16 because this layer is identical to the second layer (S2) but has 16 feature maps.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/fourth_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ef30f",
   "metadata": {},
   "source": [
    "**Fifth Layer**\n",
    "\n",
    "With 120 feature maps, each measuring 1 x 1, the fifth layer (C5) is a fully connected convolutional layer. All 400 nodes (5x5x16) in layer four, S4, are connected to each of the 120 units in C5’s 120 units.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/fifth_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b84b8",
   "metadata": {},
   "source": [
    "**Sixth Layer**\n",
    "\n",
    "A fully connected layer (F6) with 84 units makes up the sixth layer.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/sixth_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b57ed",
   "metadata": {},
   "source": [
    "**Output Layer**\n",
    "\n",
    "The SoftMax output layer, which has 10 potential values and corresponds to the digits 0 to 9, is the last layer.\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/output_layer.webp\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165f6b4",
   "metadata": {},
   "source": [
    "### Summary of LeNet-5 Architecture\n",
    "\n",
    "\n",
    "<div style=\"display:flex;align-items:center;justify-content:center;\">\n",
    "    <img src=\"images/lenet5_summary.webp\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc456011",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e441fe",
   "metadata": {},
   "source": [
    "We will be implementing the LeNet-5 Architecture on the MNIST Dataset because MNIST Dataset has images of 28x28 size which have characters are written that will help to show the better implementation of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8960b15",
   "metadata": {},
   "source": [
    "**download and load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de56271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac9d4c",
   "metadata": {},
   "source": [
    "**Pre-processing and Normalizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf07c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 28, 28\n",
    "\n",
    "# Reshape the data into 4D Array\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
    "\n",
    "input_shape = (rows, cols, 1)\n",
    "\n",
    "# Set type as float32 and normalize the values to [0,1]\n",
    "x_trian = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206d0db",
   "metadata": {},
   "source": [
    "**Define LeNet-5 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dedaee",
   "metadata": {},
   "source": [
    "> It is important to highlight that each image in the MNIST data set has a size of 28 X 28 pixels so we will use the same dimensions for LeNet-5 input instead of 32 X 32 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf64ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet(input_shape):\n",
    "    # Define Sequential Model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # C1 Convolution Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "    # S2 SubSampling layer\n",
    "    model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    # C3 Convolution Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "    # S4 SubSampling Layer\n",
    "    model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    # C5 Fully Connected Layer\n",
    "    model.add(tf.keras.layers.Dense(units=120, activation='tanh'))\n",
    "\n",
    "    # Flatten th eoutput so taht we can connect it with the uflly connected layers by converting it into 1D Aray\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # FC6 Fully Connected Layers\n",
    "    model.add(tf.keras.layers.Dense(units=84, activation='tanh'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "    # Compile the Model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa52f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = build_lenet(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08b9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7977 - loss: 0.7425\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9361 - loss: 0.2129\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1499\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9648 - loss: 0.1204\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9699 - loss: 0.1001\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9743 - loss: 0.0869\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9769 - loss: 0.0755\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9804 - loss: 0.0664\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9817 - loss: 0.0614\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9841 - loss: 0.0532\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = lenet.fit(x_train, y_train, epochs=epochs, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35d727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0658\n",
      "Accuracy: 0.9835000038146973\n"
     ]
    }
   ],
   "source": [
    "loss, acc = lenet.evaluate(x_test, y_test)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c7e0221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (60000, 28, 28) (60000, 10)\n",
      "Test Data (10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28,28)\n",
    "print('Training Data', x_train.shape, y_train.shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28)\n",
    "print('Test Data', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "065b6dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG2tJREFUeJzt3X9s1PUdx/HX8aMnSHus1vZ6UlhBgSnSTQZdgzKUhtIlDIQY/LEE1MHA4gbMH6lRUbekGybOH2GyxY3qAv5aBCKZLFpsia6wUUFC3BpKulECLZOEu1KgJfSzPwg3T1rge9zx7rXPR/JN6N3303v79UuffHvXq8855wQAwBXWz3oAAEDfRIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJAdYDfF1nZ6cOHTqk9PR0+Xw+63EAAB4559Ta2qpQKKR+/bq/zulxATp06JDy8vKsxwAAXKampiYNGzas2/t7XIDS09MlnR08IyPDeBoAgFeRSER5eXnRr+fdSVqAVq9ereeff17Nzc0qKCjQK6+8okmTJl103blvu2VkZBAgAEhhF3saJSkvQnj77be1YsUKrVy5Up999pkKCgpUUlKiI0eOJOPhAAApKCkBeuGFF7Rw4ULdf//9uvHGG7VmzRoNHjxYf/zjH5PxcACAFJTwAHV0dKiurk7FxcX/f5B+/VRcXKza2trz9m9vb1ckEonZAAC9X8ID9OWXX+rMmTPKycmJuT0nJ0fNzc3n7V9RUaFAIBDdeAUcAPQN5j+IWl5ernA4HN2ampqsRwIAXAEJfxVcVlaW+vfvr5aWlpjbW1paFAwGz9vf7/fL7/cnegwAQA+X8CugtLQ0TZgwQVVVVdHbOjs7VVVVpaKiokQ/HAAgRSXl54BWrFih+fPn67vf/a4mTZqkF198UW1tbbr//vuT8XAAgBSUlADNmzdP//3vf/X000+rublZ3/72t7Vly5bzXpgAAOi7fM45Zz3EV0UiEQUCAYXDYd4JAQBS0KV+HTd/FRwAoG8iQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwOsBwAuZt26dZ7XtLW1xfVYdXV1ntf8/ve/j+uxvHrqqac8r7njjjvieqypU6fGtQ7wgisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrIf4qkgkokAgoHA4rIyMDOtxkGAPPfSQ5zW/+93vkjBJ33DjjTfGte6TTz7xvCYQCMT1WOh9LvXrOFdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJAdYDIHX1xjcW/c53vuN5zdy5cz2v2bdvn+c1r7/+uuc1X3zxhec1kvTnP//Z85oHH3wwrsdC38UVEADABAECAJhIeICeeeYZ+Xy+mG3s2LGJfhgAQIpLynNAN910kz766KP/P8gAnmoCAMRKShkGDBigYDCYjE8NAOglkvIc0L59+xQKhTRy5Ejdd999OnDgQLf7tre3KxKJxGwAgN4v4QEqLCxUZWWltmzZoldffVWNjY267bbb1Nra2uX+FRUVCgQC0S0vLy/RIwEAeqCEB6i0tFR33XWXxo8fr5KSEv3lL3/RsWPH9M4773S5f3l5ucLhcHRrampK9EgAgB4o6a8OGDp0qEaPHq2GhoYu7/f7/fL7/ckeAwDQwyT954COHz+u/fv3Kzc3N9kPBQBIIQkP0COPPKKamhr9+9//1t/+9jfdeeed6t+/v+65555EPxQAIIUl/FtwBw8e1D333KOjR4/q2muv1a233qrt27fr2muvTfRDAQBSWMID9NZbbyX6UyLJLvQy+Qt57bXXEjxJ1yZOnOh5zZYtW+J6rMGDB3tek5aW5nnNmTNnPK/p7nnUC/n00089r5GkL7/8Mq51gBe8FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLpv5AOPV+8bzzpnPO8Jp43Fv3oo488rxkyZIjnNVdSZWWl5zX/+Mc/Ej9IN2bNmnXFHgt9F1dAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMG7YUO33HJLXOvieRfttLQ0z2sGDRrkeU1P99prr3le09HRkYRJADtcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJngzUsQtEAhYj9Aj/OlPf/K85vPPP0/CJOebPn16XOtGjRqV4EmA83EFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4M1Iga/YtWuX5zU/+clPPK9pb2/3vCY3N9fzmpdeesnzGkkaOHBgXOsAL7gCAgCYIEAAABOeA7Rt2zbNnDlToVBIPp9PGzdujLnfOaenn35aubm5GjRokIqLi7Vv375EzQsA6CU8B6itrU0FBQVavXp1l/evWrVKL7/8stasWaMdO3bo6quvVklJiU6dOnXZwwIAeg/PL0IoLS1VaWlpl/c55/Tiiy/qySef1KxZsyRJb7zxhnJycrRx40bdfffdlzctAKDXSOhzQI2NjWpublZxcXH0tkAgoMLCQtXW1na5pr29XZFIJGYDAPR+CQ1Qc3OzJCknJyfm9pycnOh9X1dRUaFAIBDd8vLyEjkSAKCHMn8VXHl5ucLhcHRramqyHgkAcAUkNEDBYFCS1NLSEnN7S0tL9L6v8/v9ysjIiNkAAL1fQgOUn5+vYDCoqqqq6G2RSEQ7duxQUVFRIh8KAJDiPL8K7vjx42poaIh+3NjYqN27dyszM1PDhw/XsmXL9Mtf/lI33HCD8vPz9dRTTykUCmn27NmJnBsAkOI8B2jnzp26/fbbox+vWLFCkjR//nxVVlbqscceU1tbmxYtWqRjx47p1ltv1ZYtW3TVVVclbmoAQMrzHKCpU6fKOdft/T6fT88995yee+65yxoMsNDdjwtcSDxvLBqPxYsXe14zevToJEwCJIb5q+AAAH0TAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHh+N2wgFTzwwANxrXv77bcTPEnXli9f7nnNY489loRJADtcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJngzUvR4x48f97zmgw8+iOuxTp065XlNTk6O5zVPPPGE5zVpaWme1wA9GVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJ3owUPd5dd93lec2RI0eSMEnXfvrTn3pek5mZmYRJgNTCFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYII3I8UVVVdX53lNdXV14gfpxpw5czyvWbFiRRImAXo/roAAACYIEADAhOcAbdu2TTNnzlQoFJLP59PGjRtj7l+wYIF8Pl/MNmPGjETNCwDoJTwHqK2tTQUFBVq9enW3+8yYMUOHDx+Obm+++eZlDQkA6H08vwihtLRUpaWlF9zH7/crGAzGPRQAoPdLynNA1dXVys7O1pgxY7RkyRIdPXq0233b29sViURiNgBA75fwAM2YMUNvvPGGqqqq9Otf/1o1NTUqLS3VmTNnuty/oqJCgUAguuXl5SV6JABAD5TwnwO6++67o3+++eabNX78eI0aNUrV1dWaNm3aefuXl5fH/BxFJBIhQgDQByT9ZdgjR45UVlaWGhoaurzf7/crIyMjZgMA9H5JD9DBgwd19OhR5ebmJvuhAAApxPO34I4fPx5zNdPY2Kjdu3crMzNTmZmZevbZZzV37lwFg0Ht379fjz32mK6//nqVlJQkdHAAQGrzHKCdO3fq9ttvj3587vmb+fPn69VXX9WePXv0+uuv69ixYwqFQpo+fbp+8YtfyO/3J25qAEDK8xygqVOnyjnX7f1//etfL2sgpI6TJ096XlNeXu55TUdHh+c18ZowYYLnNWlpaUmYBOj9eC84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmEj4r+RG37FmzRrPa6qqqpIwyfkeeOCBuNZ99dfDA0guroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABM+55yzHuKrIpGIAoGAwuGwMjIyrMfBBQwaNMjzmo6OjiRMcr5wOBzXuiFDhiR4EqDvudSv41wBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmBlgPACTD8ePH41rXr1/v+jeZ3++Pa13//v09rzlz5oznNe3t7Z7XxOPkyZNxrXvppZcSPEnixPP/SJKeeOIJz2sGDhwY12NdTO/62wYASBkECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAnejBS90nXXXWc9Qo+wePHiuNaFQiHPa5qbmz2v+e1vf+t5DS5PPH83fvzjHydhEq6AAABGCBAAwISnAFVUVGjixIlKT09Xdna2Zs+erfr6+ph9Tp06pbKyMl1zzTUaMmSI5s6dq5aWloQODQBIfZ4CVFNTo7KyMm3fvl0ffvihTp8+renTp6utrS26z/Lly/X+++/r3XffVU1NjQ4dOqQ5c+YkfHAAQGrz9CKELVu2xHxcWVmp7Oxs1dXVacqUKQqHw/rDH/6g9evX64477pAkrV27Vt/61re0fft2fe9730vc5ACAlHZZzwGFw2FJUmZmpiSprq5Op0+fVnFxcXSfsWPHavjw4aqtre3yc7S3tysSicRsAIDeL+4AdXZ2atmyZZo8ebLGjRsn6ezLMNPS0jR06NCYfXNycrp9iWZFRYUCgUB0y8vLi3ckAEAKiTtAZWVl2rt3r956663LGqC8vFzhcDi6NTU1XdbnAwCkhrh+EHXp0qXavHmztm3bpmHDhkVvDwaD6ujo0LFjx2KuglpaWhQMBrv8XH6/X36/P54xAAApzNMVkHNOS5cu1YYNG7R161bl5+fH3D9hwgQNHDhQVVVV0dvq6+t14MABFRUVJWZiAECv4OkKqKysTOvXr9emTZuUnp4efV4nEAho0KBBCgQCevDBB7VixQplZmYqIyNDDz/8sIqKingFHAAghqcAvfrqq5KkqVOnxty+du1aLViwQJL0m9/8Rv369dPcuXPV3t6ukpIS3u8JAHAen3POWQ/xVZFIRIFAQOFwWBkZGdbj4ALieYPCtWvXJmES9CUDBnh/6rp///5JmKRr5/4x7sWVfIpi8uTJnteMHDnS0/6X+nWc94IDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAibh+IyogSa+99prnNVOmTPG8pqOjw/OaK+nzzz/3vKan/4qSRx991POa66+/PgmTnO+HP/yh5zXZ2dlJmASXiysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrIf4qkgkokAgoHA4rIyMDOtxAAAeXerXca6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABOeAlRRUaGJEycqPT1d2dnZmj17turr62P2mTp1qnw+X8y2ePHihA4NAEh9ngJUU1OjsrIybd++XR9++KFOnz6t6dOnq62tLWa/hQsX6vDhw9Ft1apVCR0aAJD6BnjZecuWLTEfV1ZWKjs7W3V1dZoyZUr09sGDBysYDCZmQgBAr3RZzwGFw2FJUmZmZszt69atU1ZWlsaNG6fy8nKdOHGi28/R3t6uSCQSswEAej9PV0Bf1dnZqWXLlmny5MkaN25c9PZ7771XI0aMUCgU0p49e/T444+rvr5e7733Xpefp6KiQs8++2y8YwAAUpTPOefiWbhkyRJ98MEH+uSTTzRs2LBu99u6daumTZumhoYGjRo16rz729vb1d7eHv04EokoLy9P4XBYGRkZ8YwGADAUiUQUCAQu+nU8riugpUuXavPmzdq2bdsF4yNJhYWFktRtgPx+v/x+fzxjAABSmKcAOef08MMPa8OGDaqurlZ+fv5F1+zevVuSlJubG9eAAIDeyVOAysrKtH79em3atEnp6elqbm6WJAUCAQ0aNEj79+/X+vXr9YMf/EDXXHON9uzZo+XLl2vKlCkaP358Uv4DAACpydNzQD6fr8vb165dqwULFqipqUk/+tGPtHfvXrW1tSkvL0933nmnnnzyyUt+PudSv3cIAOiZkvIc0MValZeXp5qaGi+fEgDQR/FecAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwOsB/g655wkKRKJGE8CAIjHua/f576ed6fHBai1tVWSlJeXZzwJAOBytLa2KhAIdHu/z10sUVdYZ2enDh06pPT0dPl8vpj7IpGI8vLy1NTUpIyMDKMJ7XEczuI4nMVxOIvjcFZPOA7OObW2tioUCqlfv+6f6elxV0D9+vXTsGHDLrhPRkZGnz7BzuE4nMVxOIvjcBbH4Szr43ChK59zeBECAMAEAQIAmEipAPn9fq1cuVJ+v996FFMch7M4DmdxHM7iOJyVSsehx70IAQDQN6TUFRAAoPcgQAAAEwQIAGCCAAEATKRMgFavXq1vfvObuuqqq1RYWKi///3v1iNdcc8884x8Pl/MNnbsWOuxkm7btm2aOXOmQqGQfD6fNm7cGHO/c05PP/20cnNzNWjQIBUXF2vfvn02wybRxY7DggULzjs/ZsyYYTNsklRUVGjixIlKT09Xdna2Zs+erfr6+ph9Tp06pbKyMl1zzTUaMmSI5s6dq5aWFqOJk+NSjsPUqVPPOx8WL15sNHHXUiJAb7/9tlasWKGVK1fqs88+U0FBgUpKSnTkyBHr0a64m266SYcPH45un3zyifVISdfW1qaCggKtXr26y/tXrVqll19+WWvWrNGOHTt09dVXq6SkRKdOnbrCkybXxY6DJM2YMSPm/HjzzTev4ITJV1NTo7KyMm3fvl0ffvihTp8+renTp6utrS26z/Lly/X+++/r3XffVU1NjQ4dOqQ5c+YYTp14l3IcJGnhwoUx58OqVauMJu6GSwGTJk1yZWVl0Y/PnDnjQqGQq6ioMJzqylu5cqUrKCiwHsOUJLdhw4box52dnS4YDLrnn38+etuxY8ec3+93b775psGEV8bXj4Nzzs2fP9/NmjXLZB4rR44ccZJcTU2Nc+7s//uBAwe6d999N7rPP//5TyfJ1dbWWo2ZdF8/Ds459/3vf9/97Gc/sxvqEvT4K6COjg7V1dWpuLg4elu/fv1UXFys2tpaw8ls7Nu3T6FQSCNHjtR9992nAwcOWI9kqrGxUc3NzTHnRyAQUGFhYZ88P6qrq5Wdna0xY8ZoyZIlOnr0qPVISRUOhyVJmZmZkqS6ujqdPn065nwYO3ashg8f3qvPh68fh3PWrVunrKwsjRs3TuXl5Tpx4oTFeN3qcW9G+nVffvmlzpw5o5ycnJjbc3Jy9K9//ctoKhuFhYWqrKzUmDFjdPjwYT377LO67bbbtHfvXqWnp1uPZ6K5uVmSujw/zt3XV8yYMUNz5sxRfn6+9u/fryeeeEKlpaWqra1V//79rcdLuM7OTi1btkyTJ0/WuHHjJJ09H9LS0jR06NCYfXvz+dDVcZCke++9VyNGjFAoFNKePXv0+OOPq76+Xu+9957htLF6fIDwf6WlpdE/jx8/XoWFhRoxYoTeeecdPfjgg4aToSe4++67o3+++eabNX78eI0aNUrV1dWaNm2a4WTJUVZWpr179/aJ50EvpLvjsGjRouifb775ZuXm5mratGnav3+/Ro0adaXH7FKP/xZcVlaW+vfvf96rWFpaWhQMBo2m6hmGDh2q0aNHq6GhwXoUM+fOAc6P840cOVJZWVm98vxYunSpNm/erI8//jjm17cEg0F1dHTo2LFjMfv31vOhu+PQlcLCQknqUedDjw9QWlqaJkyYoKqqquhtnZ2dqqqqUlFRkeFk9o4fP679+/crNzfXehQz+fn5CgaDMedHJBLRjh07+vz5cfDgQR09erRXnR/OOS1dulQbNmzQ1q1blZ+fH3P/hAkTNHDgwJjzob6+XgcOHOhV58PFjkNXdu/eLUk963ywfhXEpXjrrbec3+93lZWV7osvvnCLFi1yQ4cOdc3NzdajXVE///nPXXV1tWtsbHSffvqpKy4udllZWe7IkSPWoyVVa2ur27Vrl9u1a5eT5F544QW3a9cu95///Mc559yvfvUrN3ToULdp0ya3Z88eN2vWLJefn+9OnjxpPHliXeg4tLa2ukceecTV1ta6xsZG99FHH7lbbrnF3XDDDe7UqVPWoyfMkiVLXCAQcNXV1e7w4cPR7cSJE9F9Fi9e7IYPH+62bt3qdu7c6YqKilxRUZHh1Il3sePQ0NDgnnvuObdz507X2NjoNm3a5EaOHOmmTJliPHmslAiQc8698sorbvjw4S4tLc1NmjTJbd++3XqkK27evHkuNzfXpaWlueuuu87NmzfPNTQ0WI+VdB9//LGTdN42f/5859zZl2I/9dRTLicnx/n9fjdt2jRXX19vO3QSXOg4nDhxwk2fPt1de+21buDAgW7EiBFu4cKFve4faV3990tya9euje5z8uRJ99BDD7lvfOMbbvDgwe7OO+90hw8fths6CS52HA4cOOCmTJniMjMznd/vd9dff7179NFHXTgcth38a/h1DAAAEz3+OSAAQO9EgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJj4H+bPyErxrc0bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 1\n",
    "plt.imshow(x_test[image_index].reshape(28,28), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c076376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pred = lenet.predict(x_test[image_index].reshape(1,rows,cols,1))\n",
    "print(pred.argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
