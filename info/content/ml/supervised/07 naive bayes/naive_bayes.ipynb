{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;color:#0F4C81\">Naïve Bayes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayes Theorem\n",
    "- Hands-on demo\n",
    "- sklearn implementation:\n",
    "    - GaussianNB\n",
    "    - MultinomialNB\n",
    "    - BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' theorem** (alternatively **Bayes' law** or **Bayes' rule**, after Thomas Bayes) gives a mathematical rule for inverting conditional probabilities, allowing one to find the probability of a cause given its effect.[1] For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to someone of a known age to be assessed more accurately by conditioning it relative to their age, rather than assuming that the person is typical of the population as a whole.\n",
    "\n",
    "Bayes' theorem is stated mathematically as the following equation:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "where $A$ and $B$ are events and $P(B) \\ne 0$.\n",
    "- $P(A|B)$ is a **conditional probability**: the probability of event $A$ occuring given that $B$ is true. It is also called the **posterios probability** of $A$ given $B$.\n",
    "- $P(B|A)$ is also a conditional probability: the probability of event $B$ occuring given that $A$ is true. It can also be interpreted as the **likelihood** of $A$ given a fixed $B$ because $P(B|A) = L(A|B)$.\n",
    "- $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ respectively without any givene conditions. They are known as the **prior probability** and **marginal probability**.\n",
    "\n",
    "[Wikipedia article](https://en.wikipedia.org/wiki/Bayes%27_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise: Applying Bayes' Theorem to Wealth Prediction**  \n",
    "\n",
    "#### **Problem Statement**  \n",
    "Given the dataset below, determine the probability that a person is **super wealthy** (Wealth Group = 3) given that they have a **bachelor’s degree** (Academic Qualification Group = 2).  \n",
    "\n",
    "#### **Dataset**  \n",
    "\n",
    "| ID  | Name     | Academic Qualification | Group X | Wealth   | Group Y |\n",
    "|-----|---------|------------------------|---------|----------|---------|\n",
    "| 1   | Alice   | Bachelor               | 2       | $18.1B   | 3       |\n",
    "| 2   | Bob     | PhD                    | 3       | $900,000 | 2       |\n",
    "| 3   | Charlie | Master                 | 3       | $10,000  | 1       |\n",
    "| 4   | David   | Bachelor               | 2       | $25,000  | 1       |\n",
    "| 5   | Emma    | Bachelor               | 2       | $40,000  | 2       |\n",
    "| 6   | Frank   | High School            | 1       | $12,500  | 1       |\n",
    "| 7   | Grace   | High School            | 1       | $29,000  | 1       |\n",
    "| 8   | Henry   | High School            | 1       | $1,500   | 1       |\n",
    "| 9   | Ivy     | Bachelor               | 2       | $125,000 | 2       |\n",
    "| 10  | Jack    | Bachelor               | 2       | $100,000 | 2       |\n",
    "\n",
    "#### **Step 1: Define Events**  \n",
    "- $ A $ = Being **super wealthy** (Wealth Group = 3)  \n",
    "- $ B $ = Having a **bachelor’s degree** (Academic Qualification Group = 2)  \n",
    "\n",
    "We need to compute $ P(A | B) $, the probability of being super wealthy given that someone has a bachelor’s degree.  \n",
    "\n",
    "Using **Bayes’ Theorem**:  \n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "#### **Step 2: Compute Required Probabilities from Data**  \n",
    "\n",
    "- **Total number of individuals** = 10  \n",
    "- **Individuals in Wealth Group = 3 (Super Wealthy)** = **1** (Alice)  \n",
    "- **Individuals in Academic Qualification Group = 2 (Bachelor's Degree)** = **5** (Alice, David, Emma, Ivy, Jack)  \n",
    "- **Individuals who are both in Wealth Group = 3 and Academic Qualification Group = 2** = **1** (Alice)  \n",
    "\n",
    "$$\n",
    "P(A) = \\frac{\\text{Super Wealthy individuals}}{\\text{Total individuals}} = \\frac{1}{10} = 0.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B) = \\frac{\\text{Individuals with a Bachelor's degree}}{\\text{Total individuals}} = \\frac{5}{10} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B | A) = \\frac{\\text{Super Wealthy individuals with a Bachelor's degree}}{\\text{Total Super Wealthy individuals}} = \\frac{1}{1} = 1\n",
    "$$\n",
    "\n",
    "#### **Step 3: Apply Bayes’ Theorem**  \n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{(1) (0.1)}{0.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{0.1}{0.5} = 0.2\n",
    "$$\n",
    "\n",
    "Thus, **if someone has a bachelor’s degree, the probability that they are super wealthy is 20%**.\n",
    "\n",
    "\n",
    "### **Interpretation**  \n",
    "Even though the overall probability of being super wealthy is low (**10%** of the population), having a **bachelor’s degree doubles the probability to 20%**. However, since only one person in the dataset is super wealthy, this is a small sample size, and more data would be needed to draw stronger conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Academic Qualification</th>\n",
       "      <th>Group X</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Group Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2</td>\n",
       "      <td>1.810000e+10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>PhD</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000000e+05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>David</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2</td>\n",
       "      <td>2.500000e+04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Frank</td>\n",
       "      <td>High School</td>\n",
       "      <td>1</td>\n",
       "      <td>1.250000e+04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Grace</td>\n",
       "      <td>High School</td>\n",
       "      <td>1</td>\n",
       "      <td>2.900000e+04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Henry</td>\n",
       "      <td>High School</td>\n",
       "      <td>1</td>\n",
       "      <td>1.500000e+03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Ivy</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2</td>\n",
       "      <td>1.250000e+05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Name Academic Qualification  Group X        Wealth  Group Y\n",
       "0   1    Alice               Bachelor        2  1.810000e+10        3\n",
       "1   2      Bob                    PhD        3  9.000000e+05        2\n",
       "2   3  Charlie                 Master        3  1.000000e+04        1\n",
       "3   4    David               Bachelor        2  2.500000e+04        1\n",
       "4   5     Emma               Bachelor        2  4.000000e+04        2\n",
       "5   6    Frank            High School        1  1.250000e+04        1\n",
       "6   7    Grace            High School        1  2.900000e+04        1\n",
       "7   8    Henry            High School        1  1.500000e+03        1\n",
       "8   9      Ivy               Bachelor        2  1.250000e+05        2\n",
       "9  10     Jack               Bachelor        2  1.000000e+05        2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"],\n",
    "    \"Academic Qualification\": [\"Bachelor\", \"PhD\", \"Master\", \"Bachelor\", \"Bachelor\", \n",
    "                               \"High School\", \"High School\", \"High School\", \"Bachelor\", \"Bachelor\"],\n",
    "    \"Group X\": [2, 3, 3, 2, 2, 1, 1, 1, 2, 2],\n",
    "    \"Wealth\": [18.1e9, 900000, 10000, 25000, 40000, 12500, 29000, 1500, 125000, 100000],\n",
    "    \"Group Y\": [3, 2, 1, 1, 2, 1, 1, 1, 2, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Super Wealthy | Bachelor's Degree) = 0.20\n"
     ]
    }
   ],
   "source": [
    "# Count total individuals\n",
    "total_individuals = len(df)\n",
    "\n",
    "# Count individuals with a Bachelor's degree\n",
    "bachelor_group = df[df[\"Group X\"] == 2]\n",
    "count_bachelor = len(bachelor_group)\n",
    "\n",
    "# Count super wealthy individuals (Wealth Group Y = 3)\n",
    "super_wealthy = df[df[\"Group Y\"] == 3]\n",
    "count_super_wealthy = len(super_wealthy)\n",
    "\n",
    "# Count individuals who are both super wealthy and have a Bachelor's degree\n",
    "super_wealthy_bachelor = bachelor_group[bachelor_group[\"Group Y\"] == 3]\n",
    "count_super_wealthy_bachelor = len(super_wealthy_bachelor)\n",
    "\n",
    "# Calculate P(A | B)\n",
    "P_A_given_B = count_super_wealthy_bachelor / count_bachelor\n",
    "print(f\"P(Super Wealthy | Bachelor's Degree) = {P_A_given_B:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **When Should You Use Bayes' Theorem?**\n",
    "\n",
    "In this specific case, you can compute $ P(A | B) $ directly from the dataset using simple conditional probability:\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{\\text{Individuals in both Group A (super wealthy) and Group B (bachelor's degree)}}{\\text{Individuals in Group B (bachelor's degree)}}\n",
    "$$\n",
    "\n",
    "which gives us:\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{1}{5} = 0.2\n",
    "$$\n",
    "\n",
    "\n",
    "Bayes' Theorem is particularly useful in scenarios where **direct probability computation is not feasible** due to missing or indirect information. Here are some key situations where you **must** use Bayes' Theorem:\n",
    "\n",
    "#### **1. When You Have Indirect Probabilities**  \n",
    "- Sometimes, you don't have **direct data on $ P(A | B) $** but have **reverse information** like $ P(B | A) $ and need to compute $ P(A | B) $.\n",
    "- Example: **Medical Testing**\n",
    "  - You know the probability of testing positive given that someone has a disease ($ P(B | A) $).\n",
    "  - You need to compute the probability that a person actually has the disease given a positive test ($ P(A | B) $).\n",
    "\n",
    "#### **2. When You Need to Incorporate Prior Knowledge**\n",
    "- Bayes' Theorem allows you to adjust probabilities based on prior beliefs or historical data.\n",
    "- Example: **Spam Detection**\n",
    "  - If an email contains the word \"free,\" what is the probability it is spam?\n",
    "  - We may know:\n",
    "    - $ P(\\text{\"free\"} | \\text{spam}) $ = Probability of \"free\" appearing in spam.\n",
    "    - $ P(\\text{\"free\"} | \\text{not spam}) $ = Probability of \"free\" appearing in legitimate emails.\n",
    "    - $ P(\\text{spam}) $ = Overall likelihood of spam.\n",
    "  - **Bayes' Theorem helps compute the final probability that an email is spam given it contains \"free\".**\n",
    "\n",
    "#### **3. When You Have Overlapping or Confusing Events**\n",
    "- Example: **Diagnostics & Fraud Detection**\n",
    "  - You might know:\n",
    "    - Probability of an alert being triggered when fraud is present.\n",
    "    - Probability of an alert being triggered when no fraud is present (false positives).\n",
    "    - Overall probability of fraud occurring.\n",
    "  - If an alert is triggered, what is the chance fraud is actually happening? **Bayes' Theorem helps here**.\n",
    "\n",
    "### **When You Can Skip Bayes' Theorem**\n",
    "If you **already have** enough direct data to compute conditional probabilities (like in the wealth dataset), **Bayes' Theorem is not necessary**. It's most useful when you're reasoning **backwards from observed data to hidden causes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example Where Bayes' Theorem is Necessary**: Identifying a Defective Product  \n",
    "\n",
    "#### **Scenario:**  \n",
    "A factory produces **microchips** from **two different machines**:  \n",
    "- **Machine A** produces **60%** of the total microchips.  \n",
    "- **Machine B** produces **40%** of the total microchips.  \n",
    "\n",
    "The probability of a defective chip is:  \n",
    "- **Machine A:** $ P(D | A) = 2\\% = 0.02 $  \n",
    "- **Machine B:** $ P(D | B) = 5\\% = 0.05 $  \n",
    "\n",
    "If a randomly selected microchip is found to be **defective**, what is the probability that it came from **Machine B**?\n",
    "\n",
    "\n",
    "### **Step 1: Define Events**  \n",
    "- $ A $: The chip comes from **Machine A**  \n",
    "- $ B $: The chip comes from **Machine B**  \n",
    "- $ D $: The chip is **defective**  \n",
    "\n",
    "We need to compute:  \n",
    "\n",
    "$$\n",
    "P(B | D) = \\frac{P(D | B) P(B)}{P(D)}\n",
    "$$\n",
    "\n",
    "\n",
    "### **Step 2: Compute Total Probability of Defective Chips**  \n",
    "\n",
    "Using the **law of total probability**, the overall probability of getting a defective chip is:\n",
    "\n",
    "$$\n",
    "P(D) = P(D | A) P(A) + P(D | B) P(B)\n",
    "$$\n",
    "\n",
    "Substituting values:\n",
    "\n",
    "$$\n",
    "P(D) = (0.02 \\times 0.6) + (0.05 \\times 0.4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(D) = 0.012 + 0.02 = 0.032\n",
    "$$\n",
    "\n",
    "\n",
    "### **Step 3: Apply Bayes’ Theorem**  \n",
    "\n",
    "$$\n",
    "P(B | D) = \\frac{P(D | B) P(B)}{P(D)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B | D) = \\frac{(0.05 \\times 0.4)}{0.032}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B | D) = \\frac{0.02}{0.032} = 0.625\n",
    "$$\n",
    "\n",
    "So, **if a microchip is defective, there is a 62.5% probability that it came from Machine B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D) = 0.0320\n",
      "P(Machine B | Defective Chip) = 0.625\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_A = 0.6  # Probability that a chip comes from Machine A\n",
    "P_B = 0.4  # Probability that a chip comes from Machine B\n",
    "\n",
    "P_D_given_A = 0.02  # Probability of defect given Machine A\n",
    "P_D_given_B = 0.05  # Probability of defect given Machine B\n",
    "\n",
    "# Compute total probability of a defective chip using Law of Total Probability\n",
    "P_D = (P_D_given_A * P_A) + (P_D_given_B * P_B)\n",
    "\n",
    "print(f\"P(D) = {P_D:.4f}\")\n",
    "\n",
    "# Compute P(B | D) using Bayes' Theorem\n",
    "P_B_given_D = (P_D_given_B * P_B) / P_D\n",
    "\n",
    "print(f\"P(Machine B | Defective Chip) = {P_B_given_D:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Bayes' Theorem to Naive Bayes**  \n",
    "Now that we've established the foundational understanding of Bayes' Theorem and how it allows us to update our beliefs in the presence of new evidence, we can move on to an important application: **Naive Bayes**. While Bayes' Theorem works for general probabilistic reasoning, **Naive Bayes** is a specific application of this concept in classification problems, particularly when dealing with high-dimensional data. The term \"naive\" comes from the simplifying assumption that the features are **conditionally independent** given the class label, which significantly reduces the complexity of the computation. This assumption, while often unrealistic in real-world data, still allows Naive Bayes classifiers to perform surprisingly well in many practical situations. In the following sections, we will explore how different types of Naive Bayes models, such as **Gaussian**, **Multinomial**, and **Bernoulli**, extend Bayes' Theorem for classification tasks and implement them using **scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Types of Naive Bayes Classifier**\n",
    "\n",
    "There are three main types of Naive Bayes classifiers. The key difference between these types lies in the assumption they make about the distribution of features:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**: Suited for binary/boolean features. It assumes each feature is a binary-valued (0/1) variable.\n",
    "2. **Multinomial Naive Bayes**: Typically used for discrete counts. It’s often used in text classification, where features might be word counts.\n",
    "3. **Gaussian Naive Bayes**: Assumes that continuous features follow a normal distribution.\n",
    "\n",
    "<div style=\"display:flex;justify-content:center;align-items:center;\">\n",
    "<img src=\"images/naive_bayes_models.png\" style=\"width=400px;object-fit:cover;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bernoulli Naive Bayes (`BernoulliNB`)**\n",
    "\n",
    "It is a good start to focus on the simplest one which is Bernoulli NB. BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use artificial golf dataset as an example. This dataset predicts whether a person will play golf based on weather conditions:\n",
    "\n",
    "|    | Outlook   |   Temperature |   Humidity | Wind   | Play   |\n",
    "|---:|:----------|--------------:|-----------:|:-------|:-------|\n",
    "|  0 | sunny     |            85 |         85 | False  | No     |\n",
    "|  1 | sunny     |            80 |         90 | True   | No     |\n",
    "|  2 | overcast  |            83 |         78 | False  | Yes    |\n",
    "|  3 | rain      |            70 |         96 | False  | Yes    |\n",
    "|  4 | rain      |            68 |         80 | False  | Yes    |\n",
    "|  5 | rain      |            65 |         70 | True   | No     |\n",
    "|  6 | overcast  |            64 |         65 | True   | Yes    |\n",
    "|  7 | sunny     |            72 |         95 | False  | No     |\n",
    "|  8 | sunny     |            69 |         70 | False  | Yes    |\n",
    "|  9 | rain      |            75 |         80 | False  | Yes    |\n",
    "| 10 | sunny     |            75 |         70 | True   | Yes    |\n",
    "| 11 | overcast  |            72 |         90 | True   | Yes    |\n",
    "| 12 | overcast  |            81 |         75 | False  | Yes    |\n",
    "| 13 | rain      |            71 |         80 | True   | No     |\n",
    "| 14 | sunny     |            81 |         88 | True   | No     |\n",
    "| 15 | overcast  |            74 |         92 | False  | Yes    |\n",
    "| 16 | rain      |            76 |         85 | False  | Yes    |\n",
    "| 17 | sunny     |            78 |         75 | True   | No     |\n",
    "| 18 | sunny     |            82 |         92 | False  | No     |\n",
    "| 19 | rain      |            67 |         90 | True   | No     |\n",
    "| 20 | overcast  |            85 |         85 | True   | Yes    |\n",
    "| 21 | rain      |            73 |         88 | False  | Yes    |\n",
    "| 22 | sunny     |            88 |         65 | True   | Yes    |\n",
    "| 23 | overcast  |            77 |         70 | False  | Yes    |\n",
    "| 24 | sunny     |            79 |         60 | False  | Yes    |\n",
    "| 25 | overcast  |            80 |         95 | True   | Yes    |\n",
    "| 26 | rain      |            66 |         70 | False  | No     |\n",
    "| 27 | overcast  |            84 |         78 | False  | Yes    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll adapt it slightly for Bernoulli Naive Bayes by converting our features to binary.\n",
    "\n",
    "|    |   Temperature_Hot |   Humidity_Humid |   Wind |   overcast |   rain |   sunny |   Play |\n",
    "|---:|------------------:|-----------------:|-------:|-----------:|-------:|--------:|-------:|\n",
    "|  0 |                 1 |                1 |      0 |          0 |      0 |       1 |      0 |\n",
    "|  1 |                 0 |                1 |      1 |          0 |      0 |       1 |      0 |\n",
    "|  2 |                 1 |                1 |      0 |          1 |      0 |       0 |      1 |\n",
    "|  3 |                 0 |                1 |      0 |          0 |      1 |       0 |      1 |\n",
    "|  4 |                 0 |                1 |      0 |          0 |      1 |       0 |      1 |\n",
    "|  5 |                 0 |                0 |      1 |          0 |      1 |       0 |      0 |\n",
    "|  6 |                 0 |                0 |      1 |          1 |      0 |       0 |      1 |\n",
    "|  7 |                 0 |                1 |      0 |          0 |      0 |       1 |      0 |\n",
    "|  8 |                 0 |                0 |      0 |          0 |      0 |       1 |      1 |\n",
    "|  9 |                 0 |                1 |      0 |          0 |      1 |       0 |      1 |\n",
    "| 10 |                 0 |                0 |      1 |          0 |      0 |       1 |      1 |\n",
    "| 11 |                 0 |                1 |      1 |          1 |      0 |       0 |      1 |\n",
    "| 12 |                 1 |                0 |      0 |          1 |      0 |       0 |      1 |\n",
    "| 13 |                 0 |                1 |      1 |          0 |      1 |       0 |      0 |\n",
    "| 14 |                 1 |                1 |      1 |          0 |      0 |       1 |      0 |\n",
    "| 15 |                 0 |                1 |      0 |          1 |      0 |       0 |      1 |\n",
    "| 16 |                 0 |                1 |      0 |          0 |      1 |       0 |      1 |\n",
    "| 17 |                 0 |                0 |      1 |          0 |      0 |       1 |      0 |\n",
    "| 18 |                 1 |                1 |      0 |          0 |      0 |       1 |      0 |\n",
    "| 19 |                 0 |                1 |      1 |          0 |      1 |       0 |      0 |\n",
    "| 20 |                 1 |                1 |      1 |          1 |      0 |       0 |      1 |\n",
    "| 21 |                 0 |                1 |      0 |          0 |      1 |       0 |      1 |\n",
    "| 22 |                 1 |                0 |      1 |          0 |      0 |       1 |      1 |\n",
    "| 23 |                 0 |                0 |      0 |          1 |      0 |       0 |      1 |\n",
    "| 24 |                 0 |                0 |      0 |          0 |      0 |       1 |      1 |\n",
    "| 25 |                 0 |                1 |      1 |          1 |      0 |       0 |      1 |\n",
    "| 26 |                 0 |                0 |      0 |          0 |      1 |       0 |      0 |\n",
    "| 27 |                 1 |                1 |      0 |          1 |      0 |       0 |      1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main Mechanism**\n",
    "Bernoulli Naive Bayes operates on data where each feature is either 0 or 1.\n",
    "\n",
    "1. Calculate the probability of each class in the training data.\n",
    "2. For each feature and class, calculate the probability of the feature being 1 and 0 given the class.\n",
    "3. For a new instance: For each class, multiply its probability by the probability of each feature value (0 or 1) for that class.\n",
    "4. Predict the class with the highest resulting probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Calculate Prior Probabilities**\n",
    "The **prior probability** of a class $ C_k $ is calculated as:  \n",
    "\n",
    "$$\n",
    "P(C_k) = \\frac{\\text{Number of instances in class } C_k}{\\text{Total instances}}\n",
    "$$\n",
    "\n",
    "From the dataset, we count how many times **Play = Yes** and **Play = No**:\n",
    "\n",
    "- **Yes (Play = 1)** → 17 instances\n",
    "- **No (Play = 0)** → 10 instances\n",
    "- **Total instances** → $ 27 $\n",
    "\n",
    "$$\n",
    "P(\\text{Play=Yes}) = \\frac{17}{27} \\approx 0.63\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Play=No}) = \\frac{10}{27} \\approx 0.37\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 2: Compute Likelihoods**\n",
    "For each feature (binary column), we compute:\n",
    "\n",
    "$$\n",
    "P(X_i = 1 | C_k) = \\frac{\\text{Number of instances where } X_i = 1 \\text{ and } C_k}{\\text{Total instances where } C_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X_i = 0 | C_k) = 1 - P(X_i = 1 | C_k)\n",
    "$$\n",
    "\n",
    "We compute these probabilities for **each feature** (e.g., `Temperature_Hot`, `Humidity_Humid`, etc.) **given Play = Yes and Play = No**.\n",
    "\n",
    "### **Example Calculation for `Sunny` Feature**\n",
    "- **Sunny = 1 | Play = Yes** → Appears in **5 out of 17** cases.\n",
    "\n",
    "$$\n",
    "P(\\text{Sunny=1 | Play=Yes}) = \\frac{5}{17} \\approx 0.29\n",
    "$$\n",
    "\n",
    "- **Sunny = 1 | Play = No** → Appears in **5 out of 10** cases.\n",
    "\n",
    "$$\n",
    "P(\\text{Sunny=1 | Play=No}) = \\frac{5}{10} = 0.50\n",
    "$$\n",
    "\n",
    "We repeat this for all features.\n",
    "\n",
    "## **Step 3: Apply Bayes Theorem**\n",
    "For a **new test instance**:\n",
    "> **(Temperature_Hot = 1, Humidity_Humid = 1, Wind = 0, Overcast = 0, Rain = 0, Sunny = 1)**  \n",
    "> (i.e., **Hot, Humid, No Wind, Sunny**)\n",
    "\n",
    "We use **Bayes’ Rule** to calculate:\n",
    "\n",
    "$$\n",
    "P(\\text{Play=Yes} | X) = \\frac{P(X | \\text{Play=Yes}) P(\\text{Play=Yes})}{P(X)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Play=No} | X) = \\frac{P(X | \\text{Play=No}) P(\\text{Play=No})}{P(X)}\n",
    "$$\n",
    "\n",
    "Since we only care about comparing probabilities, we ignore **P(X)** and compute the numerator.\n",
    "\n",
    "$$\n",
    "P(\\text{Play=Yes} | X) \\propto P(X | \\text{Play=Yes}) P(\\text{Play=Yes})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Play=No} | X) \\propto P(X | \\text{Play=No}) P(\\text{Play=No})\n",
    "$$\n",
    "\n",
    "### **Compute Likelihood for Play = Yes**\n",
    "$$\n",
    "P(X | \\text{Play=Yes}) = P(\\text{TempHot=1} | \\text{Play=Yes}) \\times P(\\text{HumidityHumid=1} | \\text{Play=Yes}) \\times P(\\text{Wind=0} | \\text{Play=Yes}) \\times P(\\text{Sunny=1} | \\text{Play=Yes})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.35) \\times (0.47) \\times (0.59) \\times (0.29)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.028\n",
    "$$\n",
    "\n",
    "Final probability:\n",
    "\n",
    "$$\n",
    "P(\\text{Play=Yes} | X) = 0.028 \\times 0.63 = 0.0176\n",
    "$$\n",
    "\n",
    "### **Compute Likelihood for Play = No**\n",
    "$$\n",
    "P(X | \\text{Play=No}) = P(\\text{TempHot=1} | \\text{Play=No}) \\times P(\\text{HumidityHumid=1} | \\text{Play=No}) \\times P(\\text{Wind=0} | \\text{Play=No}) \\times P(\\text{Sunny=1} | \\text{Play=No})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.50) \\times (0.70) \\times (0.40) \\times (0.50)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.07\n",
    "$$\n",
    "\n",
    "Final probability:\n",
    "\n",
    "$$\n",
    "P(\\text{Play=No} | X) = 0.07 \\times 0.37 = 0.0259\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 4: Make a Prediction**\n",
    "Since:\n",
    "\n",
    "$$\n",
    "P(\\text{Play=Yes} | X) = 0.0176\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Play=No} | X) = 0.0259\n",
    "$$\n",
    "\n",
    "Since $ P(\\text{Play=No} | X) > P(\\text{Play=Yes} | X) $, we predict **Play = No** (won't play golf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "dataset_dict = {\n",
    "    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n",
    "    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n",
    "    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n",
    "    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n",
    "    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "df = pd.DataFrame(dataset_dict)\n",
    "\n",
    "# Prepare data for model\n",
    "df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)\n",
    "df['Wind'] = df['Wind'].astype(int)\n",
    "df['Play'] = (df['Play'] == 'Yes').astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X, y = df.drop(columns='Play'), df['Play']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n",
    "\n",
    "# Scale numerical features (for automatic binarization)\n",
    "scaler = StandardScaler()\n",
    "float_cols = X_train.select_dtypes(include=['float64']).columns\n",
    "X_train[float_cols] = scaler.fit_transform(X_train[float_cols])\n",
    "X_test[float_cols] = scaler.transform(X_test[float_cols])\n",
    "\n",
    "# Train the model\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "# Check accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gaussian Naive Bayes (GaussianNB)**\n",
    "\n",
    "**Overview**\n",
    "\n",
    "Gaussian Naive Bayes is a classification algorithm based on **Bayes' Theorem** with the assumption that the **features are conditionally independent** given the class label (the \"naive\" assumption). What makes **Gaussian Naive Bayes (GaussianNB)** unique is that it assumes that the continuous features follow a **Gaussian (normal) distribution**.\n",
    "\n",
    "### **How Gaussian Naive Bayes Works**\n",
    "\n",
    "1. **Assumptions**:  \n",
    "   Gaussian Naive Bayes assumes that:\n",
    "   - Each feature (variable) is independent of the others within a given class (the Naive assumption).\n",
    "   - Each feature follows a **Gaussian (normal) distribution** within each class.\n",
    "\n",
    "2. **Bayes' Theorem**:  \n",
    "   The algorithm relies on **Bayes' Theorem** to calculate the posterior probability of a class given the features:\n",
    "\n",
    "   $$\n",
    "   P(C_k | X) = \\frac{P(X | C_k) \\cdot P(C_k)}{P(X)}\n",
    "   $$\n",
    "   where:\n",
    "   - $ C_k $ is the class.\n",
    "   - $ X $ represents the feature vector.\n",
    "   - $ P(C_k | X) $ is the posterior probability of class $ C_k $ given the feature vector $ X $.\n",
    "   - $ P(X | C_k) $ is the likelihood, the probability of observing $ X $ given class $ C_k $.\n",
    "   - $ P(C_k) $ is the prior probability of class $ C_k $.\n",
    "   - $ P(X) $ is the marginal likelihood of the feature vector $ X $.\n",
    "\n",
    "   Since we are dealing with continuous features, the likelihood $ P(X | C_k) $ is modeled as the **product of Gaussian distributions** for each feature.\n",
    "\n",
    "3. **Gaussian Distribution for Each Feature**:  \n",
    "   Each feature $ x_i $ is assumed to follow a normal distribution, given the class $ C_k $:\n",
    "\n",
    "   $$\n",
    "   P(x_i | C_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
    "   $$\n",
    "   where:\n",
    "   - $ \\mu $ is the mean of the feature $ x_i $ within class $ C_k $.\n",
    "   - $ \\sigma^2 $ is the variance of the feature $ x_i $ within class $ C_k $.\n",
    "   \n",
    "   The likelihood $ P(X | C_k) $ is the product of these Gaussian probabilities for each feature, assuming the features are independent:\n",
    "\n",
    "   $$\n",
    "   P(X | C_k) = \\prod_{i=1}^{n} P(x_i | C_k)\n",
    "   $$\n",
    "\n",
    "4. **Posterior Probability**:  \n",
    "   After calculating the likelihood $ P(X | C_k) $ and using **Bayes' Theorem**, the algorithm calculates the posterior probability for each class $ C_k $. The class with the highest posterior probability is chosen as the predicted class.\n",
    "\n",
    "   $$\n",
    "   P(C_k | X) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)\n",
    "   $$\n",
    "\n",
    "   The class with the highest posterior probability is selected as the predicted label for the given input vector $ X $.\n",
    "\n",
    "### **Key Steps in Gaussian Naive Bayes**\n",
    "\n",
    "1. **Estimate Parameters**:  \n",
    "   For each class $ C_k $, calculate:\n",
    "   - The mean $ \\mu_k $ of each feature in class $ C_k $.\n",
    "   - The variance $ \\sigma_k^2 $ of each feature in class $ C_k $.\n",
    "\n",
    "2. **Compute Likelihood**:  \n",
    "   For each test point, compute the likelihood of each feature $ x_i $ given each class using the **Gaussian distribution**.\n",
    "\n",
    "3. **Apply Bayes’ Theorem**:  \n",
    "   Combine the prior probability of each class $ P(C_k) $, the likelihood of the features $ P(X | C_k) $, and the normalizing constant to compute the posterior probability $ P(C_k | X) $.\n",
    "\n",
    "4. **Predict**:  \n",
    "   Select the class $ C_k $ with the highest posterior probability.\n",
    "\n",
    "### **When to Use Gaussian Naive Bayes**\n",
    "\n",
    "1. **Continuous Features**:  \n",
    "   Gaussian Naive Bayes is particularly useful when the features are continuous and you assume that they follow a **normal distribution**. For example, in applications where features such as height, weight, temperature, etc., are important, Gaussian Naive Bayes is a good fit.\n",
    "\n",
    "2. **Large Datasets**:  \n",
    "   Naive Bayes models, including Gaussian Naive Bayes, are computationally efficient and scale well with large datasets. This is especially true when the number of features is high.\n",
    "\n",
    "3. **Simple and Fast Classification**:  \n",
    "   Gaussian Naive Bayes is a **simple and fast** algorithm, especially when you need a baseline model or need to process data quickly for exploratory analysis. It can perform surprisingly well even when the assumptions of independence or normality are not fully met.\n",
    "\n",
    "4. **When Assumptions Hold**:  \n",
    "   It performs best when the continuous features in the dataset **roughly follow a Gaussian distribution**. If the features deviate significantly from normality, the performance might degrade.\n",
    "\n",
    "### **Advantages of Gaussian Naive Bayes**\n",
    "\n",
    "- **Fast to Train and Predict**: Since it makes strong assumptions (like feature independence and normality), it is computationally efficient, especially for large datasets.\n",
    "- **Simple and Easy to Implement**: The algorithm is simple and interpretable, making it easy to implement and understand.\n",
    "- **Handles Multi-Class Problems**: Naive Bayes can handle **multi-class classification** problems out of the box.\n",
    "\n",
    "### **Disadvantages of Gaussian Naive Bayes**\n",
    "\n",
    "- **Assumption of Feature Independence**: The \"naive\" assumption that all features are independent given the class is often unrealistic in real-world data, which can lead to suboptimal performance when this assumption is violated.\n",
    "- **Sensitivity to Imbalanced Data**: Like many probabilistic models, Gaussian Naive Bayes can be sensitive to class imbalance, where it may favor the majority class.\n",
    "- **Assumption of Gaussian Distribution**: If the features do not follow a Gaussian distribution, the model might not perform well, as it is based on that assumption.\n",
    "\n",
    "### **When Not to Use Gaussian Naive Bayes**\n",
    "- When the features **do not follow a Gaussian distribution**, or the distribution is highly skewed.\n",
    "- When the **feature independence assumption** is clearly violated (e.g., when features are strongly correlated with each other).\n",
    "- When you have **imbalanced data** and are particularly concerned with predicting the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hands-on demo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Define the Problem**\n",
    "We will classify a new data point $ X = (5.5, 3.5) $ into one of two classes:  \n",
    "- **Class 0**  \n",
    "- **Class 1**  \n",
    "\n",
    "Given a training dataset:\n",
    "\n",
    "| Sample | Feature 1 (X1) | Feature 2 (X2) | Class (Y) |\n",
    "|--------|--------------|--------------|------------|\n",
    "| 1      | 5.1          | 3.5          | 0          |\n",
    "| 2      | 4.9          | 3.0          | 0          |\n",
    "| 3      | 4.7          | 3.2          | 0          |\n",
    "| 4      | 4.6          | 3.1          | 0          |\n",
    "| 5      | 5.0          | 3.6          | 1          |\n",
    "| 6      | 5.4          | 3.9          | 1          |\n",
    "| 7      | 4.8          | 3.4          | 1          |\n",
    "| 8      | 5.2          | 3.7          | 1          |\n",
    "\n",
    "We will classify $ X = (5.5, 3.5) $ using **Gaussian Naive Bayes**.\n",
    "\n",
    "\n",
    "## **Step 2: Compute Class Priors** $ P(Y) $\n",
    "We calculate the probability of each class occurring in the dataset:\n",
    "\n",
    "$$\n",
    "P(Y=0) = \\frac{\\text{Number of samples in Class 0}}{\\text{Total number of samples}} = \\frac{4}{8} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y=1) = \\frac{\\text{Number of samples in Class 1}}{\\text{Total number of samples}} = \\frac{4}{8} = 0.5\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 3: Compute Mean and Variance for Each Feature Per Class**\n",
    "We calculate the mean $ \\mu $ and variance $ \\sigma^2 $ for **each feature per class**.\n",
    "\n",
    "### **For Class 0**\n",
    "#### Feature 1 (X1)\n",
    "$$\n",
    "\\mu_{0,1} = \\frac{5.1 + 4.9 + 4.7 + 4.6}{4} = \\frac{19.3}{4} = 4.825\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_{0,1} = \\frac{(5.1 - 4.825)^2 + (4.9 - 4.825)^2 + (4.7 - 4.825)^2 + (4.6 - 4.825)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(0.275)^2 + (0.075)^2 + (-0.125)^2 + (-0.225)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.0756 + 0.0056 + 0.0156 + 0.0506}{4} = \\frac{0.1474}{4} = 0.03685\n",
    "$$\n",
    "\n",
    "#### Feature 2 (X2)\n",
    "$$\n",
    "\\mu_{0,2} = \\frac{3.5 + 3.0 + 3.2 + 3.1}{4} = \\frac{12.8}{4} = 3.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_{0,2} = \\frac{(3.5 - 3.2)^2 + (3.0 - 3.2)^2 + (3.2 - 3.2)^2 + (3.1 - 3.2)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(0.3)^2 + (-0.2)^2 + (0)^2 + (-0.1)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.09 + 0.04 + 0 + 0.01}{4} = \\frac{0.14}{4} = 0.035\n",
    "$$\n",
    "\n",
    "\n",
    "### **For Class 1**\n",
    "#### Feature 1 (X1)\n",
    "$$\n",
    "\\mu_{1,1} = \\frac{5.0 + 5.4 + 4.8 + 5.2}{4} = \\frac{20.4}{4} = 5.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_{1,1} = \\frac{(5.0 - 5.1)^2 + (5.4 - 5.1)^2 + (4.8 - 5.1)^2 + (5.2 - 5.1)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(-0.1)^2 + (0.3)^2 + (-0.3)^2 + (0.1)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.01 + 0.09 + 0.09 + 0.01}{4} = \\frac{0.2}{4} = 0.05\n",
    "$$\n",
    "\n",
    "#### Feature 2 (X2)\n",
    "$$\n",
    "\\mu_{1,2} = \\frac{3.6 + 3.9 + 3.4 + 3.7}{4} = \\frac{14.6}{4} = 3.65\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_{1,2} = \\frac{(3.6 - 3.65)^2 + (3.9 - 3.65)^2 + (3.4 - 3.65)^2 + (3.7 - 3.65)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(-0.05)^2 + (0.25)^2 + (-0.25)^2 + (0.05)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.0025 + 0.0625 + 0.0625 + 0.0025}{4} = \\frac{0.13}{4} = 0.0325\n",
    "$$\n",
    "\n",
    "## **Step 4: Compute Likelihoods Using the Gaussian Formula**\n",
    "\n",
    "The Gaussian probability density function (PDF) is:\n",
    "\n",
    "$$\n",
    "P(X_i | Y) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "### **For Class 0**\n",
    "We have:\n",
    "- **Feature 1 (X1)**: $ \\mu_{0,1} = 4.825 $, $ \\sigma^2_{0,1} = 0.03685 $\n",
    "- **Feature 2 (X2)**: $ \\mu_{0,2} = 3.2 $, $ \\sigma^2_{0,2} = 0.035 $\n",
    "\n",
    "#### **Compute $ P(5.5 | Y=0) $**\n",
    "$$\n",
    "P(5.5 | Y=0) = \\frac{1}{\\sqrt{2 \\pi (0.03685)}} e^{-\\frac{(5.5 - 4.825)^2}{2(0.03685)}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sqrt{0.2318}} e^{-\\frac{(0.675)^2}{0.0737}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{0.4815} e^{-6.183}\n",
    "$$\n",
    "$$\n",
    "= 2.077 e^{-6.183}\n",
    "$$\n",
    "$$\n",
    "= 2.077 \\times 0.00206 = 0.0043\n",
    "$$\n",
    "\n",
    "#### **Compute $ P(3.5 | Y=0) $**\n",
    "$$\n",
    "P(3.5 | Y=0) = \\frac{1}{\\sqrt{2 \\pi (0.035)}} e^{-\\frac{(3.5 - 3.2)^2}{2(0.035)}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sqrt{0.2199}} e^{-\\frac{(0.3)^2}{0.07}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{0.469} e^{-1.286}\n",
    "$$\n",
    "$$\n",
    "= 2.131 e^{-1.286}\n",
    "$$\n",
    "$$\n",
    "= 2.131 \\times 0.276 = 0.588\n",
    "$$\n",
    "\n",
    "### **For Class 1**\n",
    "We have:\n",
    "- **Feature 1 (X1)**: $ \\mu_{1,1} = 5.1 $, $ \\sigma^2_{1,1} = 0.05 $\n",
    "- **Feature 2 (X2)**: $ \\mu_{1,2} = 3.65 $, $ \\sigma^2_{1,2} = 0.0325 $\n",
    "\n",
    "#### **Compute $ P(5.5 | Y=1) $**\n",
    "$$\n",
    "P(5.5 | Y=1) = \\frac{1}{\\sqrt{2 \\pi (0.05)}} e^{-\\frac{(5.5 - 5.1)^2}{2(0.05)}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sqrt{0.314}} e^{-\\frac{(0.4)^2}{0.1}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{0.56} e^{-1.6}\n",
    "$$\n",
    "$$\n",
    "= 1.785 e^{-1.6}\n",
    "$$\n",
    "$$\n",
    "= 1.785 \\times 0.201 = 0.359\n",
    "$$\n",
    "\n",
    "#### **Compute $ P(3.5 | Y=1) $**\n",
    "$$\n",
    "P(3.5 | Y=1) = \\frac{1}{\\sqrt{2 \\pi (0.0325)}} e^{-\\frac{(3.5 - 3.65)^2}{2(0.0325)}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sqrt{0.204}} e^{-\\frac{(-0.15)^2}{0.065}}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{0.451} e^{-0.346}\n",
    "$$\n",
    "$$\n",
    "= 2.216 e^{-0.346}\n",
    "$$\n",
    "$$\n",
    "= 2.216 \\times 0.707 = 1.566\n",
    "$$\n",
    "\n",
    "## **Step 5: Compute Posterior Probability**\n",
    "Using **Bayes' Theorem**, we compute:\n",
    "\n",
    "$$\n",
    "P(Y | X) \\propto P(X | Y) P(Y)\n",
    "$$\n",
    "\n",
    "Since $ P(Y=0) = P(Y=1) = 0.5 $, we can ignore the denominator.\n",
    "\n",
    "#### **Compute Score for Class 0**\n",
    "$$\n",
    "P(5.5 | Y=0) \\times P(3.5 | Y=0) \\times P(Y=0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.0043 \\times 0.588 \\times 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.00126\n",
    "$$\n",
    "\n",
    "#### **Compute Score for Class 1**\n",
    "$$\n",
    "P(5.5 | Y=1) \\times P(3.5 | Y=1) \\times P(Y=1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.359 \\times 1.566 \\times 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.2807\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 6: Choose the Class with the Highest Score**\n",
    "- **Class 0 Score** = **0.00126**\n",
    "- **Class 1 Score** = **0.2807**\n",
    "\n",
    "Since **Class 1 has a much higher probability**, we classify $ X = (5.5, 3.5) $ as **Class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn `GaussianNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Feature1_X1</th>\n",
       "      <th>Feature2_X2</th>\n",
       "      <th>Class_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample  Feature1_X1  Feature2_X2  Class_Y\n",
       "0       1          5.1          3.5        0\n",
       "1       2          4.9          3.0        0\n",
       "2       3          4.7          3.2        0\n",
       "3       4          4.6          3.1        0\n",
       "4       5          5.0          3.6        1\n",
       "5       6          5.4          3.9        1\n",
       "6       7          4.8          3.4        1\n",
       "7       8          5.2          3.7        1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1: Check with sklearn's GaussianNB model\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data = {\n",
    "    \"Sample\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"Feature1_X1\": [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.8, 5.2],\n",
    "    \"Feature2_X2\": [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.7],\n",
    "    \"Class_Y\": [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Feature1_X1', 'Feature2_X2']]\n",
    "y = df['Class_Y']\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "\n",
    "x_new = pd.DataFrame({\n",
    "    \"Feature1_X1\": [5.5],\n",
    "    \"Feature2_X2\": [3.5]\n",
    "})\n",
    "\n",
    "gnb.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.036875, 0.035   ],\n",
       "       [0.05    , 0.0325  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.825, 3.2  ],\n",
       "       [5.1  , 3.65 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (120, 4)\n",
      "Test set shape: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "# example 2: Classify Iris dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training & testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multinomial Naive Bayes (MultinomialNB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes (MNB) is a variant of the Naive Bayes algorithm that is designed for **discrete count data**. It is widely used for **text classification**, where features represent word frequencies or term counts in documents.\n",
    "\n",
    "\n",
    "#### **How Multinomial Naive Bayes Works**  \n",
    "Unlike Gaussian Naive Bayes, which assumes continuous features following a normal distribution, MNB works by modeling the likelihood of features using the **multinomial distribution**.\n",
    "\n",
    "Given a feature vector $ X = (X_1, X_2, ..., X_n) $, the probability of class $ Y = c $ is given by:\n",
    "\n",
    "$$\n",
    "P(Y = c | X) \\propto P(Y = c) \\prod_{i=1}^{n} P(X_i | Y = c)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "P(X_i | Y = c) = \\frac{\\text{Count}(X_i, c) + \\alpha}{\\sum_j \\text{Count}(X_j, c) + \\alpha V}\n",
    "$$\n",
    "\n",
    "- **Count(X_i, c)** = number of times feature $ X_i $ appears in class $ c $\n",
    "- **V** = total number of unique features (vocabulary size in text classification)\n",
    "- **α (Laplace smoothing)** = a smoothing parameter (usually set to 1) to avoid zero probabilities\n",
    "\n",
    "🚫 **Not suitable for continuous numerical data** (e.g., height, weight, or sensor data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spam Detection Using Multinomial Naïve Bayes (Step-by-Step by Hand, No Code)**  \n",
    "\n",
    "Multinomial Naïve Bayes is widely used for text classification, such as **spam detection**. It works by counting the frequency of words in emails and using **Bayes’ Theorem** to classify an email as **Spam or Not Spam**.\n",
    "\n",
    "\n",
    "### **Given Data (Training Emails)**  \n",
    "We will use a small dataset of **5 emails** for training.  \n",
    "\n",
    "| Email # | Text                           | Label |\n",
    "|---------|--------------------------------|-------|\n",
    "| 1       | \"Win money now\"               | Spam  |\n",
    "| 2       | \"Earn cash fast\"              | Spam  |\n",
    "| 3       | \"Hello, let’s meet today\"     | Not Spam  |\n",
    "| 4       | \"Win a free lottery ticket\"  | Spam  |\n",
    "| 5       | \"Meeting schedule update\"    | Not Spam  |\n",
    "\n",
    "\n",
    "### **Step 1: Vocabulary Extraction**  \n",
    "We extract the unique words from all emails:  \n",
    "\n",
    "**Vocabulary =** {win, money, now, earn, cash, fast, hello, let’s, meet, today, free, lottery, ticket, meeting, schedule, update}\n",
    "\n",
    "Each word will be counted in spam and non-spam emails.\n",
    "\n",
    "### **Step 2: Count Word Frequencies**\n",
    "We count how many times each word appears in **Spam** and **Not Spam** emails.\n",
    "\n",
    "| Word      | Spam Count | Not Spam Count |\n",
    "|-----------|------------|---------------|\n",
    "| win       | 2          | 0             |\n",
    "| money     | 1          | 0             |\n",
    "| now       | 1          | 0             |\n",
    "| earn      | 1          | 0             |\n",
    "| cash      | 1          | 0             |\n",
    "| fast      | 1          | 0             |\n",
    "| hello     | 0          | 1             |\n",
    "| let’s     | 0          | 1             |\n",
    "| meet      | 0          | 1             |\n",
    "| today     | 0          | 1             |\n",
    "| free      | 1          | 0             |\n",
    "| lottery   | 1          | 0             |\n",
    "| ticket    | 1          | 0             |\n",
    "| meeting   | 0          | 1             |\n",
    "| schedule  | 0          | 1             |\n",
    "| update    | 0          | 1             |\n",
    "\n",
    "\n",
    "### **Step 3: Compute Probabilities**\n",
    "#### **(1) Prior Probabilities**\n",
    "We calculate the probability of an email being **Spam** or **Not Spam** based on the training data.\n",
    "\n",
    "$$\n",
    "P(\\text{Spam}) = \\frac{\\text{Number of Spam Emails}}{\\text{Total Emails}} = \\frac{3}{5} = 0.6\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{NotSpam}) = \\frac{\\text{Number of Not Spam Emails}}{\\text{Total Emails}} = \\frac{2}{5} = 0.4\n",
    "$$\n",
    "\n",
    "#### **(2) Likelihood (Word Probabilities)**\n",
    "Using **Laplace Smoothing** ($ +1 $ to all counts to avoid zero probabilities):\n",
    "\n",
    "$$\n",
    "P(word | Spam) = \\frac{\\text{Word Count in Spam} + 1}{\\text{Total Words in Spam + Vocabulary Size}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(word | NotSpam) = \\frac{\\text{Word Count in Not Spam} + 1}{\\text{Total Words in Not Spam + Vocabulary Size}}\n",
    "$$\n",
    "\n",
    "Let's assume:  \n",
    "- Total words in **Spam** emails: **10**  \n",
    "- Total words in **Not Spam** emails: **6**  \n",
    "- Vocabulary size: **16**  \n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "P(win | Spam) = \\frac{2 + 1}{10 + 16} = \\frac{3}{26} = 0.115\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(win | Not Spam) = \\frac{0 + 1}{6 + 16} = \\frac{1}{22} = 0.045\n",
    "$$\n",
    "\n",
    "We repeat this for all words.\n",
    "\n",
    "\n",
    "### **Step 4: Classify a New Email**\n",
    "Let's classify:  \n",
    "**\"Win cash fast\"**  \n",
    "\n",
    "Using **Naïve Bayes**, we compute:\n",
    "\n",
    "#### **(1) Compute Probability for Spam**\n",
    "$$\n",
    "P(Spam | \\text{\"Win cash fast\"}) \\propto P(Spam) \\times P(win | Spam) \\times P(cash | Spam) \\times P(fast | Spam)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.6 \\times 0.115 \\times 0.077 \\times 0.077\n",
    "$$\n",
    "\n",
    "#### **(2) Compute Probability for Not Spam**\n",
    "$$\n",
    "P(NotSpam | \\text{\"Win cash fast\"}) \\propto P(NotSpam) \\times P(win | NotSpam) \\times P(cash | NotSpam) \\times P(fast | NotSpam)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.4 \\times 0.045 \\times 0.045 \\times 0.045\n",
    "$$\n",
    "\n",
    "\n",
    "### **Step 5: Compare Probabilities**\n",
    "Since **$ P(Spam | \"Win cash fast\") > P(NotSpam | \"Win cash fast\") $**,  \n",
    "we classify this email as **SPAM**.\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "- **Step 1:** Extract vocabulary.  \n",
    "- **Step 2:** Count word frequencies in Spam & Not Spam emails.  \n",
    "- **Step 3:** Compute **prior probabilities** & **word likelihoods**.  \n",
    "- **Step 4:** Compute probabilities for a new email using Bayes’ Theorem.  \n",
    "- **Step 5:** Compare and classify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultinomialNB` for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9650\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       1.00      0.74      0.85       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.87      0.91      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n",
      "Message: Congra wotulations! Youn a free lottery. Click here to claim your prize! → Prediction: Spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Select only the necessary columns\n",
    "df = df[['Category', 'Message']]\n",
    "df.columns = ['label', 'text']  # Rename columns\n",
    "\n",
    "# Convert labels to numeric: 'spam' -> 1, 'ham' -> 0\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Text preprocessing function\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()  # Convert to lowercase\n",
    "#     text = \"\".join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "#     words = text.split()  # Tokenization\n",
    "#     words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "#     return \" \".join(words)\n",
    "\n",
    "# Apply text cleaning\n",
    "# df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])  # Feature matrix\n",
    "y = df['label']  # Target variable\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Multinomial Naïve Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to predict new messages\n",
    "def predict_message(msg):\n",
    "    # msg_clean = clean_text(msg)  # Preprocess message\n",
    "    msg_vectorized = vectorizer.transform([msg])  # Convert to TF-IDF vector\n",
    "    prediction = model.predict(msg_vectorized)[0]\n",
    "    return \"Spam\" if prediction == 1 else \"Ham\"\n",
    "\n",
    "# Test with a new message\n",
    "new_message = \"Congra wotulations! Youn a free lottery. Click here to claim your prize!\"\n",
    "print(f\"Message: {new_message} → Prediction: {predict_message(new_message)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/towards-data-science/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
