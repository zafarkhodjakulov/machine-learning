### Machine Learning Algorithms For Content

- **Preprocessing**

  - Scaling
  - Normalization
- **Linear Regression**
- **Logistic Regression**
- **Gradient Descend**
- **Decision Tree**
- **Support Vector Machines**
- **Random Forest**
- **K-Nearest Neighbour Classification**
- **K-Fold Cross Validation**
- **K Means Clustering Algorithm**
- **Naive Bayes Classifier Algorithm**
- **L1 and L2 Regularization**
- **Principal Component Analysis**
- **Ensemble Learning**
- **Outlier Detection**
- **Hyperparameter Tuning**
- **Save the model**

---

Comprehensive list of data transformation algorithms:

### **1. Scaling and Normalization**

Used to adjust the range and distribution of numerical features.

- **Min-Max Scaling**: Scales data to a range [0, 1] or any custom range.
- **Standardization (Z-Score Normalization)**: Centers data around zero with a standard deviation of one.
- **Robust Scaling**: Scales using the median and interquartile range, robust to outliers.
- **Log Transformation**: Applies a logarithmic function to reduce skewness in data.
- **Box-Cox Transformation**: Transforms data to make it closer to a normal distribution.
- **L2 Normalization**: Scales feature vectors such that their Euclidean norm equals 1.


### **2. Encoding Categorical Variables**

Converts categorical data into numerical representations.

- **One-Hot Encoding**: Creates binary columns for each category.
- **Label Encoding**: Assigns a unique integer to each category.
- **Ordinal Encoding**: Encodes categories with a meaningful ordinal relationship.
- **Frequency Encoding**: Replaces categories with their frequency counts.
- **Target Encoding**: Encodes categories with the mean of the target variable.
- **Binary Encoding**: Combines hashing and one-hot encoding by converting categories into binary numbers.


### **3. Text Data Transformation**

Prepares textual data for NLP models, including LLMs.

- **Tokenization**: Splits text into words, subwords, or characters.
- **Stemming and Lemmatization**: Reduces words to their root form.
- **TF-IDF Transformation**: Converts text to numerical features based on term frequency-inverse document frequency.
- **Word Embeddings**: Maps words to dense vector representations (e.g., Word2Vec, GloVe).
- **Sentence Embeddings**: Represents sentences as dense vectors (e.g., BERT embeddings).
- **Bag of Words (BoW)**: Represents text as a matrix of word frequencies.
- **N-Grams**: Captures context by grouping consecutive words/characters.


### **4. Feature Engineering**

Derives new features from raw data.

- **Polynomial Features**: Creates interaction terms and higher-order terms.
- **Discretization**: Converts continuous features into categorical bins.
- **Feature Scaling for Time Series**: Applies techniques like differencing and seasonal decomposition.
- **Feature Crossing**: Combines features to capture interactions.
- **Principal Component Analysis (PCA)**: Reduces dimensionality while retaining variance.
- **Independent Component Analysis (ICA)**: Identifies statistically independent components in data.
- **t-SNE/UMAP**: Reduces dimensions for visualization while preserving local structure.


### **5. Handling Missing Values**

Fills or drops missing values in datasets.

- **Mean/Median/Mode Imputation**: Replaces missing values with statistical measures.
- **K-Nearest Neighbors (KNN) Imputation**: Predicts missing values based on similar rows.
- **Multiple Imputation**: Generates multiple predictions for missing data.
- **Forward/Backward Filling**: Uses neighboring values to fill gaps (common in time series).
- **Indicator Variables**: Adds a binary flag for missingness.


### **6. Handling Imbalanced Data**

Balances class distribution in target labels.

- **Over-Sampling**: Adds duplicate or synthetic samples for minority classes (e.g., SMOTE).
- **Under-Sampling**: Removes samples from majority classes.
- **Class Weighting**: Adjusts weights during model training.
- **Synthetic Data Generation**: Creates new samples for minority classes.


### **7. Time Series Transformations**

Prepares sequential data for analysis.

- **Lag Features**: Creates features based on past values.
- **Rolling Statistics**: Calculates rolling averages or standard deviations.
- **Fourier Transforms**: Analyzes frequency components in time series.
- **Seasonal Decomposition**: Breaks data into trend, seasonality, and residuals.


### **8. Data Augmentation**

Generates additional samples to improve model robustness.

- **Image Augmentation**: Applies transformations like rotation, scaling, flipping, or cropping.
- **Text Augmentation**: Introduces paraphrasing, synonym replacement, or back-translation.
- **Audio Augmentation**: Applies techniques like pitch shift, noise addition, or time-stretching.


### **9. Advanced Transformations**

Used for complex or structured data.

- **Fourier Transforms**: Converts signals to frequency domain.
- **Wavelet Transforms**: Analyzes localized changes in signals.
- **Autoencoders**: Compresses data into a lower-dimensional representation.
- **Graph Embeddings**: Maps graph structures to vector space (e.g., Node2Vec).
- **Sparse Matrix Representations**: Efficiently handles sparse datasets.


### **10. Image Data Transformation**

Prepares images for computer vision tasks.

- **Normalization**: Rescales pixel values to a standard range (e.g., [0, 1]).
- **Resize and Cropping**: Standardizes image dimensions.
- **Grayscale Conversion**: Reduces color channels for simplicity.
- **Edge Detection**: Extracts edges (e.g., using Sobel or Canny methods).


### **11. Feature Selection Techniques**

Selects relevant features to improve model performance.

- **Variance Thresholding**: Removes features with low variance.
- **Correlation Filtering**: Drops highly correlated features.
- **Recursive Feature Elimination (RFE)**: Iteratively selects features by importance.
- **L1 Regularization (Lasso)**: Shrinks coefficients of less important features to zero.

---
### Regression Algorithms:

- Linear Regression
- Polynomial Regression
- Poisson Regression
- Ordinary Least Squares (OLS) Regression
- Ordinal Regression
- Support Vector Regression
- Gradient Descent Regression
- Stepwise Regression
- Lasso Regression
- Ridge Regression
- Elastic Net Regression
- Bayesian Linear Regression
- Least-Angled Regression (LARS)
- Neural Network Regression
- Locally Estimated Scatterplot Smoothing (LOESS)
- Multivariate Adaptive Regression Splines (MARS)
- Locally Weighted Regression (LWL)
- Quantile Regression
- Principal Component Regression (PCR)
- Partial Least Squares Regression

---

### Machine Learning Algorithms
### **1. Supervised Learning Algorithms**
These algorithms are trained on labeled data, meaning that each training example is paired with an output label.

- **Linear Regression**
- **Logistic Regression**
- **Support Vector Machines (SVM)**
- **K-Nearest Neighbors (KNN)**
- **Decision Trees**
- **Random Forests**
- **Gradient Boosting Machines (GBM)**
  - XGBoost
  - LightGBM
  - CatBoost
- **Naive Bayes**
  - Gaussian Naive Bayes
  - Multinomial Naive Bayes
  - Bernoulli Naive Bayes
- **Ridge Regression**
- **Lasso Regression**

### **2. Unsupervised Learning Algorithms**
These algorithms work with unlabeled data and try to find hidden patterns or intrinsic structures.

- **K-Means Clustering**
- **Hierarchical Clustering**
- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
- **Gaussian Mixture Models (GMM)**
- **Principal Component Analysis (PCA)**
- **Independent Component Analysis (ICA)**
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**
- **Autoencoders**
- **Apriori Algorithm (for Association Rule Learning)**
- **FP-Growth (Frequent Pattern Growth)**

### **3. Semi-Supervised Learning Algorithms**
These algorithms use a small amount of labeled data combined with a large amount of unlabeled data.

- **Self-Training**
- **Co-Training**
- **Multi-View Learning**
- **Generative Adversarial Networks (for semi-supervised tasks)**
- **Label Propagation**

### **4. Reinforcement Learning Algorithms**
These algorithms learn by interacting with an environment to maximize a reward.

- **Q-Learning**
- **Deep Q-Networks (DQN)**
- **SARSA (State-Action-Reward-State-Action)**
- **Policy Gradient Methods**
- **Proximal Policy Optimization (PPO)**
- **Actor-Critic Methods**
- **Monte Carlo Tree Search (MCTS)**
- **Deep Deterministic Policy Gradient (DDPG)**

### **5. Deep Learning Algorithms**
Deep learning models are neural networks with multiple layers that can model complex patterns.

- **Convolutional Neural Networks (CNNs)** (used for image data)
- **Recurrent Neural Networks (RNNs)**
  - Long Short-Term Memory Networks (LSTM)
  - Gated Recurrent Units (GRU)
- **Transformer Networks**
  - BERT (Bidirectional Encoder Representations from Transformers)
  - GPT (Generative Pretrained Transformer)
- **Generative Adversarial Networks (GANs)**
- **Deep Belief Networks (DBN)**
- **Restricted Boltzmann Machines (RBM)**

### **6. Ensemble Learning Algorithms**
Ensemble methods combine the predictions of several models to improve accuracy.

- **Bagging**
  - Bootstrap Aggregating
  - Random Forests (can be considered a type of bagging)
- **Boosting**
  - AdaBoost
  - Gradient Boosting
  - XGBoost
  - LightGBM
  - CatBoost
- **Stacking**
- **Blending**

### **7. Dimensionality Reduction Algorithms**
These algorithms reduce the number of input variables in a dataset.

- **Principal Component Analysis (PCA)**
- **Linear Discriminant Analysis (LDA)**
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**
- **UMAP (Uniform Manifold Approximation and Projection)**

### **8. Anomaly Detection Algorithms**
Algorithms to detect unusual patterns that do not conform to expected behavior.

- **One-Class SVM**
- **Isolation Forest**
- **Local Outlier Factor (LOF)**
- **Autoencoders for anomaly detection**

### **9. Recommendation Algorithms**
Used to make recommendations, often based on collaborative filtering or content-based filtering.

- **Collaborative Filtering**
  - Matrix Factorization
  - Singular Value Decomposition (SVD)
- **Content-Based Filtering**
- **Hybrid Models**

### **10. Evolutionary Algorithms**
Inspired by natural selection processes to find optimal solutions.

- **Genetic Algorithms**
- **Particle Swarm Optimization**
- **Ant Colony Optimization**
